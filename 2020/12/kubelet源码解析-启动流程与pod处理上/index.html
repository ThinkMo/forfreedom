<!DOCTYPE html>
<html lang="en-us">
  <head>
    
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="generator" content="Hugo 0.79.0 with theme Tranquilpeak 0.4.8-BETA">
<meta name="author" content="游">
<meta name="keywords" content="k8s, kubelet">
<meta name="description" content="kubelet介绍 在k8s集群中的每个节点上都运行着一个kubelet服务进程，其主要负责向apiserver注册节点、管理pod及pod中的容器，并通过 cAdvisor 监控节点和容器的资源。
 节点管理：节点注册、节点状态更新(定期心跳) pod管理：接受来自apiserver、file、http等PodSpec，并确保这些 PodSpec 中描述的容器处于运行状态且运行状况良好 容器健康检查：通过ReadinessProbe、LivenessProbe两种探针检查容器健康状态 资源监控：通过 cAdvisor 获取其所在节点及容器的监控数据  kubelet组件模块 kubelet组件模块如上图所示，下面先对几个比较重要的几个模块进行简单说明：
 Pleg(Pod Lifecycle Event Generator) 是kubelet 的核心模块，PLEG周期性调用container runtime获取本节点containers/sandboxes的信息(像docker ps)，并与自身维护的pods cache信息进行对比，生成对应的 PodLifecycleEvent并发送到plegCh中，在kubelet syncLoop中对plegCh进行消费处理，最终达到用户的期望状态，相关设计可以看这里 podManager提供存储、访问Pod信息的接口，维护static pod和mirror pod的映射关系 containerManager 管理容器的各种资源，比如 CGroups、QoS、cpuset、device 等 KubeletGenericRuntimeManager是容器运行时的管理者，负责于 CRI 交互，完成容器和镜像的管理；关于client/server container runtime 设计可以看这里 statusManager负责维护pod状态信息并负责同步到apiserver probeManager负责探测pod状态，依赖statusManager、statusManager、livenessManager、startupManager cAdvisor是google开源的容器监控工具，集成在kubelet中，收集节点与容器的监控信息，对外提供查询接口 volumeManager 管理容器的存储卷，比如格式化资盘、挂载到 Node 本地、最后再将挂载路径传给容器  深入kubelet工作原理 从上图可以看出kubelet的主要工作逻辑在SyncLoop控制循环中，处理pod更新事件、pod生命周期变化、周期sync事件、定时清理事件；SyncLoop旁还有处理其他逻辑的控制循环，例如处理pod状态同步的statusManager，处理健康检查的probeManager等。
下面将从kubelet源码来分析各个环节的处理逻辑。
kubelet代码(版本1.19)主要位于cmd/kubelet与pkg/kubelet下，首先看下kubelet启动流程与初始化
1 cmd/kubelet/kubelet.go:34
func main() { rand.Seed(time.Now().UnixNano()) command := app.NewKubeletCommand() logs.InitLogs() defer logs.FlushLogs() if err := command.Execute(); err !">


<meta property="og:description" content="kubelet介绍 在k8s集群中的每个节点上都运行着一个kubelet服务进程，其主要负责向apiserver注册节点、管理pod及pod中的容器，并通过 cAdvisor 监控节点和容器的资源。
 节点管理：节点注册、节点状态更新(定期心跳) pod管理：接受来自apiserver、file、http等PodSpec，并确保这些 PodSpec 中描述的容器处于运行状态且运行状况良好 容器健康检查：通过ReadinessProbe、LivenessProbe两种探针检查容器健康状态 资源监控：通过 cAdvisor 获取其所在节点及容器的监控数据  kubelet组件模块 kubelet组件模块如上图所示，下面先对几个比较重要的几个模块进行简单说明：
 Pleg(Pod Lifecycle Event Generator) 是kubelet 的核心模块，PLEG周期性调用container runtime获取本节点containers/sandboxes的信息(像docker ps)，并与自身维护的pods cache信息进行对比，生成对应的 PodLifecycleEvent并发送到plegCh中，在kubelet syncLoop中对plegCh进行消费处理，最终达到用户的期望状态，相关设计可以看这里 podManager提供存储、访问Pod信息的接口，维护static pod和mirror pod的映射关系 containerManager 管理容器的各种资源，比如 CGroups、QoS、cpuset、device 等 KubeletGenericRuntimeManager是容器运行时的管理者，负责于 CRI 交互，完成容器和镜像的管理；关于client/server container runtime 设计可以看这里 statusManager负责维护pod状态信息并负责同步到apiserver probeManager负责探测pod状态，依赖statusManager、statusManager、livenessManager、startupManager cAdvisor是google开源的容器监控工具，集成在kubelet中，收集节点与容器的监控信息，对外提供查询接口 volumeManager 管理容器的存储卷，比如格式化资盘、挂载到 Node 本地、最后再将挂载路径传给容器  深入kubelet工作原理 从上图可以看出kubelet的主要工作逻辑在SyncLoop控制循环中，处理pod更新事件、pod生命周期变化、周期sync事件、定时清理事件；SyncLoop旁还有处理其他逻辑的控制循环，例如处理pod状态同步的statusManager，处理健康检查的probeManager等。
下面将从kubelet源码来分析各个环节的处理逻辑。
kubelet代码(版本1.19)主要位于cmd/kubelet与pkg/kubelet下，首先看下kubelet启动流程与初始化
1 cmd/kubelet/kubelet.go:34
func main() { rand.Seed(time.Now().UnixNano()) command := app.NewKubeletCommand() logs.InitLogs() defer logs.FlushLogs() if err := command.Execute(); err !">
<meta property="og:type" content="article">
<meta property="og:title" content="kubelet源码解析-启动流程与POD处理(上)">
<meta name="twitter:title" content="kubelet源码解析-启动流程与POD处理(上)">
<meta property="og:url" content="https://itech.red/2020/12/kubelet%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B%E4%B8%8Epod%E5%A4%84%E7%90%86%E4%B8%8A/">
<meta property="twitter:url" content="https://itech.red/2020/12/kubelet%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B%E4%B8%8Epod%E5%A4%84%E7%90%86%E4%B8%8A/">
<meta property="og:site_name" content="平凡世界">
<meta property="og:description" content="kubelet介绍 在k8s集群中的每个节点上都运行着一个kubelet服务进程，其主要负责向apiserver注册节点、管理pod及pod中的容器，并通过 cAdvisor 监控节点和容器的资源。
 节点管理：节点注册、节点状态更新(定期心跳) pod管理：接受来自apiserver、file、http等PodSpec，并确保这些 PodSpec 中描述的容器处于运行状态且运行状况良好 容器健康检查：通过ReadinessProbe、LivenessProbe两种探针检查容器健康状态 资源监控：通过 cAdvisor 获取其所在节点及容器的监控数据  kubelet组件模块 kubelet组件模块如上图所示，下面先对几个比较重要的几个模块进行简单说明：
 Pleg(Pod Lifecycle Event Generator) 是kubelet 的核心模块，PLEG周期性调用container runtime获取本节点containers/sandboxes的信息(像docker ps)，并与自身维护的pods cache信息进行对比，生成对应的 PodLifecycleEvent并发送到plegCh中，在kubelet syncLoop中对plegCh进行消费处理，最终达到用户的期望状态，相关设计可以看这里 podManager提供存储、访问Pod信息的接口，维护static pod和mirror pod的映射关系 containerManager 管理容器的各种资源，比如 CGroups、QoS、cpuset、device 等 KubeletGenericRuntimeManager是容器运行时的管理者，负责于 CRI 交互，完成容器和镜像的管理；关于client/server container runtime 设计可以看这里 statusManager负责维护pod状态信息并负责同步到apiserver probeManager负责探测pod状态，依赖statusManager、statusManager、livenessManager、startupManager cAdvisor是google开源的容器监控工具，集成在kubelet中，收集节点与容器的监控信息，对外提供查询接口 volumeManager 管理容器的存储卷，比如格式化资盘、挂载到 Node 本地、最后再将挂载路径传给容器  深入kubelet工作原理 从上图可以看出kubelet的主要工作逻辑在SyncLoop控制循环中，处理pod更新事件、pod生命周期变化、周期sync事件、定时清理事件；SyncLoop旁还有处理其他逻辑的控制循环，例如处理pod状态同步的statusManager，处理健康检查的probeManager等。
下面将从kubelet源码来分析各个环节的处理逻辑。
kubelet代码(版本1.19)主要位于cmd/kubelet与pkg/kubelet下，首先看下kubelet启动流程与初始化
1 cmd/kubelet/kubelet.go:34
func main() { rand.Seed(time.Now().UnixNano()) command := app.NewKubeletCommand() logs.InitLogs() defer logs.FlushLogs() if err := command.Execute(); err !">
<meta name="twitter:description" content="kubelet介绍 在k8s集群中的每个节点上都运行着一个kubelet服务进程，其主要负责向apiserver注册节点、管理pod及pod中的容器，并通过 cAdvisor 监控节点和容器的资源。
 节点管理：节点注册、节点状态更新(定期心跳) pod管理：接受来自apiserver、file、http等PodSpec，并确保这些 PodSpec 中描述的容器处于运行状态且运行状况良好 容器健康检查：通过ReadinessProbe、LivenessProbe两种探针检查容器健康状态 资源监控：通过 cAdvisor 获取其所在节点及容器的监控数据  kubelet组件模块 kubelet组件模块如上图所示，下面先对几个比较重要的几个模块进行简单说明：
 Pleg(Pod Lifecycle Event Generator) 是kubelet 的核心模块，PLEG周期性调用container runtime获取本节点containers/sandboxes的信息(像docker ps)，并与自身维护的pods cache信息进行对比，生成对应的 PodLifecycleEvent并发送到plegCh中，在kubelet syncLoop中对plegCh进行消费处理，最终达到用户的期望状态，相关设计可以看这里 podManager提供存储、访问Pod信息的接口，维护static pod和mirror pod的映射关系 containerManager 管理容器的各种资源，比如 CGroups、QoS、cpuset、device 等 KubeletGenericRuntimeManager是容器运行时的管理者，负责于 CRI 交互，完成容器和镜像的管理；关于client/server container runtime 设计可以看这里 statusManager负责维护pod状态信息并负责同步到apiserver probeManager负责探测pod状态，依赖statusManager、statusManager、livenessManager、startupManager cAdvisor是google开源的容器监控工具，集成在kubelet中，收集节点与容器的监控信息，对外提供查询接口 volumeManager 管理容器的存储卷，比如格式化资盘、挂载到 Node 本地、最后再将挂载路径传给容器  深入kubelet工作原理 从上图可以看出kubelet的主要工作逻辑在SyncLoop控制循环中，处理pod更新事件、pod生命周期变化、周期sync事件、定时清理事件；SyncLoop旁还有处理其他逻辑的控制循环，例如处理pod状态同步的statusManager，处理健康检查的probeManager等。
下面将从kubelet源码来分析各个环节的处理逻辑。
kubelet代码(版本1.19)主要位于cmd/kubelet与pkg/kubelet下，首先看下kubelet启动流程与初始化
1 cmd/kubelet/kubelet.go:34
func main() { rand.Seed(time.Now().UnixNano()) command := app.NewKubeletCommand() logs.InitLogs() defer logs.FlushLogs() if err := command.Execute(); err !">
<meta property="og:locale" content="zh-cn">

  
    <meta property="article:published_time" content="2020-12-31T17:13:30">
  
  
    <meta property="article:modified_time" content="2020-12-31T17:13:30">
  
  
  
    
      <meta property="article:section" content="k8s">
    
  
  


<meta name="twitter:card" content="summary">











  <meta property="og:image" content="https://avatars3.githubusercontent.com/u/6330242?v=3&s=460">
  <meta property="twitter:image" content="https://avatars3.githubusercontent.com/u/6330242?v=3&s=460">


    <title>kubelet源码解析-启动流程与POD处理(上)</title>

    <link rel="icon" href="https://itech.red/favicon.png">
    

    

    <link rel="canonical" href="https://itech.red/2020/12/kubelet%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B%E4%B8%8Epod%E5%A4%84%E7%90%86%E4%B8%8A/">

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css" integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css" integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin="anonymous" />
    
    
    <link rel="stylesheet" href="https://itech.red/css/style-twzjdbqhmnnacqs0pwwdzcdbt8yhv8giawvjqjmyfoqnvazl0dalmnhdkvp7.min.css" />
    
    

    
      
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-82870469-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

    
    
  </head>

  <body>
    <div id="blog">
      <header id="header" data-behavior="5">
  <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
  <div class="header-title">
    <a class="header-title-link" href="https://itech.red/">平凡世界</a>
  </div>
  
    
      <a class="header-right-picture "
         href="https://itech.red/#about">
    
    
    
      
        <img class="header-picture" src="https://avatars3.githubusercontent.com/u/6330242?v=3&amp;s=460" alt="作者的图片" />
      
    
    </a>
  
</header>

      <nav id="sidebar" data-behavior="5">
  <div class="sidebar-container">
    
      <div class="sidebar-profile">
        <a href="https://itech.red/#about">
          <img class="sidebar-profile-picture" src="https://avatars3.githubusercontent.com/u/6330242?v=3&amp;s=460" alt="作者的图片" />
        </a>
        <h4 class="sidebar-profile-name">游</h4>
        
          <h5 class="sidebar-profile-bio">沉下心，多坚持</h5>
        
      </div>
    
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://itech.red/">
    
      <i class="sidebar-button-icon fa fa-lg fa-home"></i>
      
      <span class="sidebar-button-desc">首页</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://itech.red/categories">
    
      <i class="sidebar-button-icon fa fa-lg fa-bookmark"></i>
      
      <span class="sidebar-button-desc">分类</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://itech.red/#about">
    
      <i class="sidebar-button-icon fa fa-lg fa-question"></i>
      
      <span class="sidebar-button-desc">关于</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://itech.red/external_link">
    
      <i class="sidebar-button-icon fa fa-lg fa-external-link"></i>
      
      <span class="sidebar-button-desc">友链</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      

    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://itech.red/index.xml">
    
      <i class="sidebar-button-icon fa fa-lg fa-rss"></i>
      
      <span class="sidebar-button-desc">RSS</span>
    </a>
  </li>


    </ul>
  </div>
</nav>

      

      <div id="main" data-behavior="5"
        class="
               hasCoverMetaIn
               ">
        <article class="post" itemscope itemType="http://schema.org/BlogPosting">
          
          
            <div class="post-header main-content-wrap text-left">
  
    <h1 class="post-title" itemprop="headline">
      kubelet源码解析-启动流程与POD处理(上)
    </h1>
  
  
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2020-12-31T17:13:30&#43;08:00">
        
  
  
  
  
    2020-12-31 17:13:30
  

      </time>
    
    
  
  
    <span>发布在</span>
    
      <a class="category-link" href="https://itech.red/categories/k8s">k8s</a>
    
  

  </div>

</div>
          
          <div class="post-content markdown" itemprop="articleBody">
            <div class="main-content-wrap">
              <h3 id="kubelet介绍">kubelet介绍</h3>
<p>在k8s集群中的每个节点上都运行着一个kubelet服务进程，其主要负责向apiserver注册节点、管理pod及pod中的容器，并通过 cAdvisor 监控节点和容器的资源。</p>
<ul>
<li>节点管理：节点注册、节点状态更新(定期心跳)</li>
<li>pod管理：接受来自apiserver、file、http等PodSpec，并确保这些 PodSpec 中描述的容器处于运行状态且运行状况良好</li>
<li>容器健康检查：通过ReadinessProbe、LivenessProbe两种探针检查容器健康状态</li>
<li>资源监控：通过 cAdvisor 获取其所在节点及容器的监控数据</li>
</ul>
<h3 id="kubelet组件模块">kubelet组件模块</h3>
<p><img src="https://itech.red/images/kubelet_mods.png" alt="kubelet_mods"></p>
<p>kubelet组件模块如上图所示，下面先对几个比较重要的几个模块进行简单说明：</p>
<ul>
<li>Pleg(Pod Lifecycle Event Generator) 是kubelet 的核心模块，PLEG周期性调用container runtime获取本节点containers/sandboxes的信息(像docker ps)，并与自身维护的pods cache信息进行对比，生成对应的 PodLifecycleEvent并发送到plegCh中，在kubelet syncLoop中对plegCh进行消费处理，最终达到用户的期望状态，相关设计可以看<a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/pod-lifecycle-event-generator.md">这里</a></li>
<li>podManager提供存储、访问Pod信息的接口，维护static pod和mirror pod的映射关系</li>
<li>containerManager 管理容器的各种资源，比如 CGroups、QoS、cpuset、device 等</li>
<li>KubeletGenericRuntimeManager是容器运行时的管理者，负责于 CRI 交互，完成容器和镜像的管理；关于client/server container runtime 设计可以看<a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/runtime-client-server.md">这里</a></li>
<li>statusManager负责维护pod状态信息并负责同步到apiserver</li>
<li>probeManager负责探测pod状态，依赖statusManager、statusManager、livenessManager、startupManager</li>
<li>cAdvisor是google开源的容器监控工具，集成在kubelet中，收集节点与容器的监控信息，对外提供查询接口</li>
<li>volumeManager 管理容器的存储卷，比如格式化资盘、挂载到 Node 本地、最后再将挂载路径传给容器</li>
</ul>
<h3 id="深入kubelet工作原理">深入kubelet工作原理</h3>
<p><img src="https://itech.red/images/kubelet.png" alt="kubelet"></p>
<p>从上图可以看出kubelet的主要工作逻辑在SyncLoop控制循环中，处理pod更新事件、pod生命周期变化、周期sync事件、定时清理事件；SyncLoop旁还有处理其他逻辑的控制循环，例如处理pod状态同步的statusManager，处理健康检查的probeManager等。</p>
<p>下面将从kubelet源码来分析各个环节的处理逻辑。</p>
<p>kubelet代码(版本1.19)主要位于cmd/kubelet与pkg/kubelet下，首先看下kubelet启动流程与初始化</p>
<p><img src="https://itech.red/images/kubelet_up.png" alt="kubelet_up"></p>
<p>1 cmd/kubelet/kubelet.go:34</p>
<pre><code>func main() {
	rand.Seed(time.Now().UnixNano())

	command := app.NewKubeletCommand()
	logs.InitLogs()
	defer logs.FlushLogs()

	if err := command.Execute(); err != nil {
		os.Exit(1)
	}
}
</code></pre><p><strong>main</strong>  入口函数，kubelet依赖cobra命令行库，创建kubelet command并执行</p>
<p>2  cmd/kubelet/app/server.go:113</p>
<pre><code>func NewKubeletCommand() *cobra.Command {
	cleanFlagSet := pflag.NewFlagSet(componentKubelet, pflag.ContinueOnError)
	cleanFlagSet.SetNormalizeFunc(cliflag.WordSepNormalizeFunc)
	// kubelet包含两部分配置
	// kubeletFlags是不能在运行时修改的配置或不在node间共享的配置
	// kubeletConfig可以在node间共享的配置，可以动态配置
	kubeletFlags := options.NewKubeletFlags()
	kubeletConfig, err := options.NewKubeletConfiguration()
	if err != nil {
		klog.Fatal(err)
	}

	cmd := &amp;cobra.Command{
		Use: componentKubelet,
		...
		Run: func(cmd *cobra.Command, args []string) {
			// 命令行参数解析
			if err := cleanFlagSet.Parse(args); err != nil {
				cmd.Usage()
				klog.Fatal(err)
			}
			...
			// 验证参数
			if err := options.ValidateKubeletFlags(kubeletFlags); err != nil {
				klog.Fatal(err)
			}
			// 加载kubelet配置文件
			if configFile := kubeletFlags.KubeletConfigFile; len(configFile) &gt; 0 {
				kubeletConfig, err = loadConfigFile(configFile)
				...
			}
			// 验证配置文件
			if err := kubeletconfigvalidation.ValidateKubeletConfiguration(kubeletConfig); err != nil {
				klog.Fatal(err)
			}
			// 构建KubeletServer
			kubeletServer := &amp;options.KubeletServer{
				KubeletFlags:         *kubeletFlags,
				KubeletConfiguration: *kubeletConfig,
			}
			// 创建KubeletDeps
			kubeletDeps, err := UnsecuredDependencies(kubeletServer, utilfeature.DefaultFeatureGate)
			if err != nil {
				klog.Fatal(err)
			}
			// 设置信号处理
			ctx := genericapiserver.SetupSignalContext()
			// 执行Run
			klog.V(5).Infof(&quot;KubeletConfiguration: %#v&quot;, kubeletServer.KubeletConfiguration)
			if err := Run(ctx, kubeletServer, kubeletDeps, utilfeature.DefaultFeatureGate); err != nil {
				klog.Fatal(err)
			}
		},
	}
    ......
    return cmd
}
</code></pre><p><strong>NewKubeletCommand</strong>创建kubelet cobra.Command结构体，Run函数主要解析参数、配置文件，构建KubeletServer、KubeletDependencies，设置信号处理函数，最后执行Run</p>
<p>3  cmd/kubelet/app/server.go:472</p>
<pre><code>func run(ctx context.Context, s *options.KubeletServer, kubeDeps *kubelet.Dependencies, featureGate featuregate.FeatureGate) (err error) {
    ......
	// 验证KubeletServer中flags、configs参数
	if err := options.ValidateKubeletServer(s); err != nil {
		return err
	}
	// 获得Kubelet Lock File，feature
	if s.ExitOnLockContention &amp;&amp; s.LockFilePath == &quot;&quot; {
		return errors.New(&quot;cannot exit on lock file contention: no lock file specified&quot;)
	}
	done := make(chan struct{})
	if s.LockFilePath != &quot;&quot; {
		klog.Infof(&quot;acquiring file lock on %q&quot;, s.LockFilePath)
		if err := flock.Acquire(s.LockFilePath); err != nil {
			return fmt.Errorf(&quot;unable to acquire file lock on %q: %v&quot;, s.LockFilePath, err)
		}
		if s.ExitOnLockContention {
			klog.Infof(&quot;watching for inotify events for: %v&quot;, s.LockFilePath)
			if err := watchForLockfileContention(s.LockFilePath, done); err != nil {
				return err
			}
		}
	}
	// 将当前配置注册到http server /configz URL
	err = initConfigz(&amp;s.KubeletConfiguration)
	if err != nil {
		klog.Errorf(&quot;unable to register KubeletConfiguration with configz, error: %v&quot;, err)
	}
	......
	// 如果是standalone mode将所有client设置为nil
	switch {
	case standaloneMode:
		kubeDeps.KubeClient = nil
		kubeDeps.EventClient = nil
		kubeDeps.HeartbeatClient = nil
		klog.Warningf(&quot;standalone mode, no API client&quot;)
    // 根据配置创建各个clientset
	case kubeDeps.KubeClient == nil, kubeDeps.EventClient == nil, kubeDeps.HeartbeatClient == nil:
		clientConfig, closeAllConns, err := buildKubeletClientConfig(ctx, s, nodeName)
		if err != nil {
			return err
		}
		if closeAllConns == nil {
			return errors.New(&quot;closeAllConns must be a valid function other than nil&quot;)
		}
		kubeDeps.OnHeartbeatFailure = closeAllConns

		kubeDeps.KubeClient, err = clientset.NewForConfig(clientConfig)
		if err != nil {
			return fmt.Errorf(&quot;failed to initialize kubelet client: %v&quot;, err)
		}

		// make a separate client for events
		eventClientConfig := *clientConfig
		eventClientConfig.QPS = float32(s.EventRecordQPS)
		eventClientConfig.Burst = int(s.EventBurst)
		kubeDeps.EventClient, err = v1core.NewForConfig(&amp;eventClientConfig)
		if err != nil {
			return fmt.Errorf(&quot;failed to initialize kubelet event client: %v&quot;, err)
		}

		// make a separate client for heartbeat with throttling disabled and a timeout attached
		heartbeatClientConfig := *clientConfig
		heartbeatClientConfig.Timeout = s.KubeletConfiguration.NodeStatusUpdateFrequency.Duration
		// The timeout is the minimum of the lease duration and status update frequency
		leaseTimeout := time.Duration(s.KubeletConfiguration.NodeLeaseDurationSeconds) * time.Second
		if heartbeatClientConfig.Timeout &gt; leaseTimeout {
			heartbeatClientConfig.Timeout = leaseTimeout
		}

		heartbeatClientConfig.QPS = float32(-1)
		kubeDeps.HeartbeatClient, err = clientset.NewForConfig(&amp;heartbeatClientConfig)
		if err != nil {
			return fmt.Errorf(&quot;failed to initialize kubelet heartbeat client: %v&quot;, err)
		}
	}
    // 初始化auth
	if kubeDeps.Auth == nil {
		auth, runAuthenticatorCAReload, err := BuildAuth(nodeName, kubeDeps.KubeClient, s.KubeletConfiguration)
		if err != nil {
			return err
		}
		kubeDeps.Auth = auth
		runAuthenticatorCAReload(ctx.Done())
	}
    // 设置cgroupRoot
	var cgroupRoots []string
	nodeAllocatableRoot := cm.NodeAllocatableRoot(s.CgroupRoot, s.CgroupsPerQOS, s.CgroupDriver)
	cgroupRoots = append(cgroupRoots, nodeAllocatableRoot)
	kubeletCgroup, err := cm.GetKubeletContainer(s.KubeletCgroups)
	if err != nil {
		klog.Warningf(&quot;failed to get the kubelet's cgroup: %v.  Kubelet system container metrics may be missing.&quot;, err)
	} else if kubeletCgroup != &quot;&quot; {
		cgroupRoots = append(cgroupRoots, kubeletCgroup)
	}

	runtimeCgroup, err := cm.GetRuntimeContainer(s.ContainerRuntime, s.RuntimeCgroups)
	if err != nil {
		klog.Warningf(&quot;failed to get the container runtime's cgroup: %v. Runtime system container metrics may be missing.&quot;, err)
	} else if runtimeCgroup != &quot;&quot; {
		// RuntimeCgroups is optional, so ignore if it isn't specified
		cgroupRoots = append(cgroupRoots, runtimeCgroup)
	}

	if s.SystemCgroups != &quot;&quot; {
		// SystemCgroups is optional, so ignore if it isn't specified
		cgroupRoots = append(cgroupRoots, s.SystemCgroups)
	}
    // 初始化cAdvisor
	if kubeDeps.CAdvisorInterface == nil {
		imageFsInfoProvider := cadvisor.NewImageFsInfoProvider(s.ContainerRuntime, s.RemoteRuntimeEndpoint)
		kubeDeps.CAdvisorInterface, err = cadvisor.New(imageFsInfoProvider, s.RootDirectory, cgroupRoots, cadvisor.UsingLegacyCadvisorStats(s.ContainerRuntime, s.RemoteRuntimeEndpoint))
		if err != nil {
			return err
		}
	}

	// Setup event recorder if required.
	makeEventRecorder(kubeDeps, nodeName)
    // 初始化containerManager
	if kubeDeps.ContainerManager == nil {
		if s.CgroupsPerQOS &amp;&amp; s.CgroupRoot == &quot;&quot; {
			klog.Info(&quot;--cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /&quot;)
			s.CgroupRoot = &quot;/&quot;
		}
       // 计算节点capacity
		var reservedSystemCPUs cpuset.CPUSet
		var errParse error
		if s.ReservedSystemCPUs != &quot;&quot; {
			reservedSystemCPUs, errParse = cpuset.Parse(s.ReservedSystemCPUs)
			if errParse != nil {
				// invalid cpu list is provided, set reservedSystemCPUs to empty, so it won't overwrite kubeReserved/systemReserved
				klog.Infof(&quot;Invalid ReservedSystemCPUs \&quot;%s\&quot;&quot;, s.ReservedSystemCPUs)
				return errParse
			}
			// is it safe do use CAdvisor here ??
			machineInfo, err := kubeDeps.CAdvisorInterface.MachineInfo()
			if err != nil {
				// if can't use CAdvisor here, fall back to non-explicit cpu list behavor
				klog.Warning(&quot;Failed to get MachineInfo, set reservedSystemCPUs to empty&quot;)
				reservedSystemCPUs = cpuset.NewCPUSet()
			} else {
				reservedList := reservedSystemCPUs.ToSlice()
				first := reservedList[0]
				last := reservedList[len(reservedList)-1]
				if first &lt; 0 || last &gt;= machineInfo.NumCores {
					// the specified cpuset is outside of the range of what the machine has
					klog.Infof(&quot;Invalid cpuset specified by --reserved-cpus&quot;)
					return fmt.Errorf(&quot;Invalid cpuset %q specified by --reserved-cpus&quot;, s.ReservedSystemCPUs)
				}
			}
		} else {
			reservedSystemCPUs = cpuset.NewCPUSet()
		}

		if reservedSystemCPUs.Size() &gt; 0 {
			// at cmd option valication phase it is tested either --system-reserved-cgroup or --kube-reserved-cgroup is specified, so overwrite should be ok
			klog.Infof(&quot;Option --reserved-cpus is specified, it will overwrite the cpu setting in KubeReserved=\&quot;%v\&quot;, SystemReserved=\&quot;%v\&quot;.&quot;, s.KubeReserved, s.SystemReserved)
			if s.KubeReserved != nil {
				delete(s.KubeReserved, &quot;cpu&quot;)
			}
			if s.SystemReserved == nil {
				s.SystemReserved = make(map[string]string)
			}
			s.SystemReserved[&quot;cpu&quot;] = strconv.Itoa(reservedSystemCPUs.Size())
			klog.Infof(&quot;After cpu setting is overwritten, KubeReserved=\&quot;%v\&quot;, SystemReserved=\&quot;%v\&quot;&quot;, s.KubeReserved, s.SystemReserved)
		}
		kubeReserved, err := parseResourceList(s.KubeReserved)
		if err != nil {
			return err
		}
		systemReserved, err := parseResourceList(s.SystemReserved)
		if err != nil {
			return err
		}
		var hardEvictionThresholds []evictionapi.Threshold
		// If the user requested to ignore eviction thresholds, then do not set valid values for hardEvictionThresholds here.
		if !s.ExperimentalNodeAllocatableIgnoreEvictionThreshold {
			hardEvictionThresholds, err = eviction.ParseThresholdConfig([]string{}, s.EvictionHard, nil, nil, nil)
			if err != nil {
				return err
			}
		}
		experimentalQOSReserved, err := cm.ParseQOSReserved(s.QOSReserved)
		if err != nil {
			return err
		}

		devicePluginEnabled := utilfeature.DefaultFeatureGate.Enabled(features.DevicePlugins)
       // 创建ContainerManager，其中包含cgroupManager、QosContainerManager、cpuManager等
		kubeDeps.ContainerManager, err = cm.NewContainerManager(
			kubeDeps.Mounter,
			kubeDeps.CAdvisorInterface,
			cm.NodeConfig{
				RuntimeCgroupsName:    s.RuntimeCgroups,
				SystemCgroupsName:     s.SystemCgroups,
				KubeletCgroupsName:    s.KubeletCgroups,
				ContainerRuntime:      s.ContainerRuntime,
				CgroupsPerQOS:         s.CgroupsPerQOS,
				CgroupRoot:            s.CgroupRoot,
				CgroupDriver:          s.CgroupDriver,
				KubeletRootDir:        s.RootDirectory,
				ProtectKernelDefaults: s.ProtectKernelDefaults,
				NodeAllocatableConfig: cm.NodeAllocatableConfig{
					KubeReservedCgroupName:   s.KubeReservedCgroup,
					SystemReservedCgroupName: s.SystemReservedCgroup,
					EnforceNodeAllocatable:   sets.NewString(s.EnforceNodeAllocatable...),
					KubeReserved:             kubeReserved,
					SystemReserved:           systemReserved,
					ReservedSystemCPUs:       reservedSystemCPUs,
					HardEvictionThresholds:   hardEvictionThresholds,
				},
				QOSReserved:                           *experimentalQOSReserved,
				ExperimentalCPUManagerPolicy:          s.CPUManagerPolicy,
				ExperimentalCPUManagerReconcilePeriod: s.CPUManagerReconcilePeriod.Duration,
				ExperimentalPodPidsLimit:              s.PodPidsLimit,
				EnforceCPULimits:                      s.CPUCFSQuota,
				CPUCFSQuotaPeriod:                     s.CPUCFSQuotaPeriod.Duration,
				ExperimentalTopologyManagerPolicy:     s.TopologyManagerPolicy,
			},
			s.FailSwapOn,
			devicePluginEnabled,
			kubeDeps.Recorder)

		if err != nil {
			return err
		}
	}
    // 检查是否root启动
	if err := checkPermissions(); err != nil {
		klog.Error(err)
	}

	utilruntime.ReallyCrash = s.ReallyCrashForTesting

	// kubelet oom分数设置
	oomAdjuster := kubeDeps.OOMAdjuster
	if err := oomAdjuster.ApplyOOMScoreAdj(0, int(s.OOMScoreAdj)); err != nil {
		klog.Warning(err)
	}
    // 初始化kubeDeps中runtimeService、runtimeImageService，如果是docker启动dockershim
	err = kubelet.PreInitRuntimeService(&amp;s.KubeletConfiguration,
		kubeDeps, &amp;s.ContainerRuntimeOptions,
		s.ContainerRuntime,
		s.RuntimeCgroups,
		s.RemoteRuntimeEndpoint,
		s.RemoteImageEndpoint,
		s.NonMasqueradeCIDR)
	if err != nil {
		return err
	}
    // 执行下一步RunKubelet
	if err := RunKubelet(s, kubeDeps, s.RunOnce); err != nil {
		return err
	}
    // 启动healthz http server
	if s.HealthzPort &gt; 0 {
		mux := http.NewServeMux()
		healthz.InstallHandler(mux)
		go wait.Until(func() {
			err := http.ListenAndServe(net.JoinHostPort(s.HealthzBindAddress, strconv.Itoa(int(s.HealthzPort))), mux)
			if err != nil {
				klog.Errorf(&quot;Starting healthz server failed: %v&quot;, err)
			}
		}, 5*time.Second, wait.NeverStop)
	}

	if s.RunOnce {
		return nil
	}

	// 通知systemd
	go daemon.SdNotify(false, &quot;READY=1&quot;)

	select {
	case &lt;-done:
		break
	case &lt;-ctx.Done():
		break
	}

	return nil
}
</code></pre><p><strong>run</strong> 主要执行配置检查与初始化工作</p>
<ul>
<li>KubeletServer参数验证</li>
<li>KubeDependencies中各种clientset初始化</li>
<li>ContainerManager创建初始化</li>
<li>启动dockershim、创建runtimeService、runtimeImageService</li>
</ul>
<p>4   cmd/kubelet/app/server.go:1071</p>
<pre><code>func RunKubelet(kubeServer *options.KubeletServer, kubeDeps *kubelet.Dependencies, runOnce bool) error {
	hostname, err := nodeutil.GetHostname(kubeServer.HostnameOverride)
	if err != nil {
		return err
	}
	// Query the cloud provider for our node name, default to hostname if kubeDeps.Cloud == nil
	nodeName, err := getNodeName(kubeDeps.Cloud, hostname)
	if err != nil {
		return err
	}
	hostnameOverridden := len(kubeServer.HostnameOverride) &gt; 0
	// Setup event recorder if required.
	makeEventRecorder(kubeDeps, nodeName)
    // 特权模式启动
	capabilities.Initialize(capabilities.Capabilities{
		AllowPrivileged: true,
	})

	credentialprovider.SetPreferredDockercfgPath(kubeServer.RootDirectory)
	klog.V(2).Infof(&quot;Using root directory: %v&quot;, kubeServer.RootDirectory)

	if kubeDeps.OSInterface == nil {
		kubeDeps.OSInterface = kubecontainer.RealOS{}
	}
	// 创建初始化Kubelet结构体
	k, err := createAndInitKubelet(&amp;kubeServer.KubeletConfiguration,
		......
		kubeServer.NodeStatusMaxImages)
	if err != nil {
		return fmt.Errorf(&quot;failed to create kubelet: %v&quot;, err)
	}

	// NewMainKubelet should have set up a pod source config if one didn't exist
	// when the builder was run. This is just a precaution.
	if kubeDeps.PodConfig == nil {
		return fmt.Errorf(&quot;failed to create kubelet, pod source config was nil&quot;)
	}
	podCfg := kubeDeps.PodConfig

	if err := rlimit.SetNumFiles(uint64(kubeServer.MaxOpenFiles)); err != nil {
		klog.Errorf(&quot;Failed to set rlimit on max file handles: %v&quot;, err)
	}

	// process pods and exit.
	if runOnce {
		if _, err := k.RunOnce(podCfg.Updates()); err != nil {
			return fmt.Errorf(&quot;runonce failed: %v&quot;, err)
		}
		klog.Info(&quot;Started kubelet as runonce&quot;)
	} else {
	    // 调用startKubelet
		startKubelet(k, podCfg, &amp;kubeServer.KubeletConfiguration, kubeDeps, kubeServer.EnableCAdvisorJSONEndpoints, kubeServer.EnableServer)
		klog.Info(&quot;Started kubelet&quot;)
	}
	return nil
}
</code></pre><p><strong>RunKubelet</strong> 创建Kubelet关键结构体，执行startKubelet，其最终通过goroutine进入Kubelet.Run</p>
<p>5  cmd/kubelet/kubelet.go:333</p>
<p>由于Kubelet结构体非常关键，先看下NewMainKubelet创建初始化Kubelet关键结构体函数</p>
<pre><code>func createAndInitKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration,
	......
	nodeStatusMaxImages int32) (k kubelet.Bootstrap, err error) {
	// 创建初始化Kubelet
	k, err = kubelet.NewMainKubelet(kubeCfg,
		......
		nodeStatusMaxImages)
	if err != nil {
		return nil, err
	}
	// 向apiserver发送kubelet启动event
	k.BirthCry()
	// 启动垃圾回收container、images
	k.StartGarbageCollection()
	return k, nil
}

func NewMainKubelet(...){
    ......
    // 创建PodConfig，watch apiserver、file、http等
	if kubeDeps.PodConfig == nil {
		var err error
		kubeDeps.PodConfig, err = makePodSourceConfig(kubeCfg, kubeDeps, nodeName)
		if err != nil {
			return nil, err
		}
	}
	......
	// 创建service informer、node、oom watcher
    var serviceLister corelisters.ServiceLister
	var serviceHasSynced cache.InformerSynced
	if kubeDeps.KubeClient != nil {
		kubeInformers := informers.NewSharedInformerFactory(kubeDeps.KubeClient, 0)
		serviceLister = kubeInformers.Core().V1().Services().Lister()
		serviceHasSynced = kubeInformers.Core().V1().Services().Informer().HasSynced
		kubeInformers.Start(wait.NeverStop)
	} else {
		serviceIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc})
		serviceLister = corelisters.NewServiceLister(serviceIndexer)
		serviceHasSynced = func() bool { return true }
	}

	nodeIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{})
	if kubeDeps.KubeClient != nil {
		fieldSelector := fields.Set{api.ObjectNameField: string(nodeName)}.AsSelector()
		nodeLW := cache.NewListWatchFromClient(kubeDeps.KubeClient.CoreV1().RESTClient(), &quot;nodes&quot;, metav1.NamespaceAll, fieldSelector)
		r := cache.NewReflector(nodeLW, &amp;v1.Node{}, nodeIndexer, 0)
		go r.Run(wait.NeverStop)
	}
	nodeLister := corelisters.NewNodeLister(nodeIndexer)

	nodeRef := &amp;v1.ObjectReference{
		Kind:      &quot;Node&quot;,
		Name:      string(nodeName),
		UID:       types.UID(nodeName),
		Namespace: &quot;&quot;,
	}

	oomWatcher, err := oomwatcher.NewWatcher(kubeDeps.Recorder)
	if err != nil {
		return nil, err
	}
	// 创建Kubelet
	klet := &amp;Kubelet{...}
	// 创建各种manager例如secret、configMap
	// 创建podManager，管理pod信息
	klet.podManager = kubepod.NewBasicPodManager(mirrorPodClient, secretManager, configMapManager)
	// 创建runtimeManager，其通过runtimeService、runtimeImageService实现对container的管理
	runtime, err := kuberuntime.NewKubeGenericRuntimeManager(...)
	// 创建pleg
	klet.pleg = pleg.NewGenericPLEG(klet.containerRuntime, plegChannelCapacity, plegRelistPeriod, klet.podCache, clock.RealClock{})
	// imageManager、probeManager、volumeManager、pluginManager等设置
	//   初始化workQueue、podWorker
	klet.reasonCache = NewReasonCache()
	klet.workQueue = queue.NewBasicWorkQueue(klet.clock)
	// 创建podWorker，syncPod为每个pod处理逻辑函数，在每个pod的goroutine中对pod更新进行同步
	klet.podWorkers = newPodWorkers(klet.syncPod, kubeDeps.Recorder, klet.workQueue, klet.resyncInterval, backOffPeriod, klet.podCache)
	......
}
</code></pre><p><strong>NewMainKubelet</strong> 创建并初始化Kubelet结构体，其依赖各种Manager、podConfig、pleg、podWorkers的初始化</p>
<p>6 pkg/kubelet/kubelet.go:1314   Kubelet.Run方法</p>
<pre><code>func (kl *Kubelet) Run(updates &lt;-chan kubetypes.PodUpdate) {
    // 初始化与runtime无关的逻辑，metrics、imageManager等
	if err := kl.initializeModules(); err != nil {
		kl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.KubeletSetupFailed, err.Error())
		klog.Fatal(err)
	}
	// 启动volume manager
	go kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop)

	if kl.kubeClient != nil {
		// 同步node状态
		go wait.Until(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, wait.NeverStop)
		go kl.fastStatusUpdateOnce()
		// 启动node lease
		go kl.nodeLeaseController.Run(wait.NeverStop)
	}
	// runtime首次初始化、定时更新
	go wait.Until(kl.updateRuntimeUp, 5*time.Second, wait.NeverStop)

	// 设置iptables规则
	if kl.makeIPTablesUtilChains {
		kl.initNetworkUtil()
	}

	// 启动podKiller，监听podKiller.podKillingCh
	go wait.Until(kl.podKiller.PerformPodKillingWork, 1*time.Second, wait.NeverStop)

	// 启动statusManager、probeManager
	kl.statusManager.Start()
	kl.probeManager.Start()

	// Start syncing RuntimeClasses if enabled.
	if kl.runtimeClassManager != nil {
		kl.runtimeClassManager.Start(wait.NeverStop)
	}

	// 启动pleg
	kl.pleg.Start()
	// 进入主循环
	kl.syncLoop(updates, kl)
}
</code></pre><p><strong>Run</strong> 实现kubelet参数、配置、信号处理注册，初始化依赖、Kubelet启动Manager, 进入各自manager处理逻辑，最后进入kubelet主循环逻辑，主循环有5个事件源</p>
<ul>
<li>configCh                    podConfig处理来自apiserver、file、http的pod更新，在创建Kubelet时初始化</li>
<li>handler                      liveness manager处理，在创建Kubelet时初始化，为statusManager的一部分</li>
<li>syncCh                      time定时同步pod</li>
<li>housekeepingCh         time定时清理pod</li>
<li>plegCh                      处理来自pleg的消息，在创建Kubelet时初始化，关于pleg可以看<a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/pod-lifecycle-event-generator.md">这里</a>，简单说对容器状态感知由每个pod worker(goroutine)轮询改为pleg</li>
</ul>
<p>7  pkg/kubelet/kubelet.go:1814  kubelet事件循环函数</p>
<pre><code>
func (kl *Kubelet) syncLoopIteration(configCh &lt;-chan kubetypes.PodUpdate, handler SyncHandler,
	syncCh &lt;-chan time.Time, housekeepingCh &lt;-chan time.Time, plegCh &lt;-chan *pleg.PodLifecycleEvent) bool {
	select {
	// 处理来自apiserver、file、http的pod增删改查，最终走到Kubelet的syncPod处理
	case u, open := &lt;-configCh:
		if !open {
			klog.Errorf(&quot;Update channel is closed. Exiting the sync loop.&quot;)
			return false
		}
		switch u.Op {
		case kubetypes.ADD:
			klog.V(2).Infof(&quot;SyncLoop (ADD, %q): %q&quot;, u.Source, format.Pods(u.Pods))
			handler.HandlePodAdditions(u.Pods)
		case kubetypes.UPDATE:
			klog.V(2).Infof(&quot;SyncLoop (UPDATE, %q): %q&quot;, u.Source, format.PodsWithDeletionTimestamps(u.Pods))
			handler.HandlePodUpdates(u.Pods)
		case kubetypes.REMOVE:
			klog.V(2).Infof(&quot;SyncLoop (REMOVE, %q): %q&quot;, u.Source, format.Pods(u.Pods))
			handler.HandlePodRemoves(u.Pods)
		case kubetypes.RECONCILE:
			klog.V(4).Infof(&quot;SyncLoop (RECONCILE, %q): %q&quot;, u.Source, format.Pods(u.Pods))
			handler.HandlePodReconcile(u.Pods)
		case kubetypes.DELETE:
			klog.V(2).Infof(&quot;SyncLoop (DELETE, %q): %q&quot;, u.Source, format.Pods(u.Pods))
			handler.HandlePodUpdates(u.Pods)
		case kubetypes.SET:
			klog.Errorf(&quot;Kubelet does not support snapshot update&quot;)
		default:
			klog.Errorf(&quot;Invalid event type received: %d.&quot;, u.Op)
		}
		kl.sourcesReady.AddSource(u.Source)
    // 处理来自runtime的容器变化
	case e := &lt;-plegCh:
		if e.Type == pleg.ContainerStarted {
			kl.lastContainerStartedTime.Add(e.ID, time.Now())
		}
		if isSyncPodWorthy(e) {
			if pod, ok := kl.podManager.GetPodByUID(e.ID); ok {
				klog.V(2).Infof(&quot;SyncLoop (PLEG): %q, event: %#v&quot;, format.Pod(pod), e)
				handler.HandlePodSyncs([]*v1.Pod{pod})
			} else {
				klog.V(4).Infof(&quot;SyncLoop (PLEG): ignore irrelevant event: %#v&quot;, e)
			}
		}
		if e.Type == pleg.ContainerDied {
			if containerID, ok := e.Data.(string); ok {
				kl.cleanUpContainersInPod(e.ID, containerID)
			}
		}
	// 同步pod状态，最终走到Kubelet的syncPod处理
	case &lt;-syncCh:
		podsToSync := kl.getPodsToSync()
		if len(podsToSync) == 0 {
			break
		}
		klog.V(4).Infof(&quot;SyncLoop (SYNC): %d pods; %s&quot;, len(podsToSync), format.Pods(podsToSync))
		handler.HandlePodSyncs(podsToSync)
	// 处理probe fail，最终走到Kubelet的syncPod处理
	case update := &lt;-kl.livenessManager.Updates():
		if update.Result == proberesults.Failure {
			pod, ok := kl.podManager.GetPodByUID(update.PodUID)
			if !ok {
				klog.V(4).Infof(&quot;SyncLoop (container unhealthy): ignore irrelevant update: %#v&quot;, update)
				break
			}
			klog.V(1).Infof(&quot;SyncLoop (container unhealthy): %q&quot;, format.Pod(pod))
			handler.HandlePodSyncs([]*v1.Pod{pod})
		}
	case &lt;-housekeepingCh:
		if !kl.sourcesReady.AllReady() {
			klog.V(4).Infof(&quot;SyncLoop (housekeeping, skipped): sources aren't ready yet.&quot;)
		} else {
			klog.V(4).Infof(&quot;SyncLoop (housekeeping)&quot;)
			if err := handler.HandlePodCleanups(); err != nil {
				klog.Errorf(&quot;Failed cleaning pods: %v&quot;, err)
			}
		}
	}
	return true
}
</code></pre><p>** syncLoopIteration** 根据事件类型调用对应的handler进行处理</p>
<p>8 pkg/kubelet/pod_workers.go:200</p>
<p>对不同事件有不同的处理handler，大部分handler都会通过dispatchWork进入podWorkers.UpdatePod</p>
<pre><code>// podWorkers存储每个pod的goroutine并处理pod的update到对应goroutine

func (p *podWorkers) UpdatePod(options *UpdatePodOptions) {
	pod := options.Pod
	uid := pod.UID
	var podUpdates chan UpdatePodOptions
	var exists bool

	p.podLock.Lock()
	defer p.podLock.Unlock()
	// pod是否已存在，如果不存在则创建channel，启动对应pod的goroutine
	if podUpdates, exists = p.podUpdates[uid]; !exists {
		podUpdates = make(chan UpdatePodOptions, 1)
		p.podUpdates[uid] = podUpdates
		go func() {
			defer runtime.HandleCrash()
			// pod处理函数
			p.managePodLoop(podUpdates)
		}()
	}
	// pod存在且空闲则写入，否则暂存最近的更新
	if !p.isWorking[pod.UID] {
		p.isWorking[pod.UID] = true
		podUpdates &lt;- *options
	} else {
		// SyncPodKill不会被覆盖
		if !found || update.UpdateType != kubetypes.SyncPodKill {
			p.lastUndeliveredWorkUpdate[pod.UID] = *options
		}
	}
}
// pod的处理goroutine，监听事件并调用Kubelet的方法syncPod处理
pkg/kubelet/pod_workers.go:158
func (p *podWorkers) managePodLoop(podUpdates &lt;-chan UpdatePodOptions) {
	var lastSyncTime time.Time
	for update := range podUpdates {
		err := func() error {
			podUID := update.Pod.UID
			status, err := p.podCache.GetNewerThan(podUID, lastSyncTime)
			if err != nil {
				p.recorder.Eventf(update.Pod, v1.EventTypeWarning, events.FailedSync, &quot;error determining status: %v&quot;, err)
				return err
			}
			// 调用Kubelet的方法syncPod
			err = p.syncPodFn(syncPodOptions{
				mirrorPod:      update.MirrorPod,
				pod:            update.Pod,
				podStatus:      status,
				killPodOptions: update.KillPodOptions,
				updateType:     update.UpdateType,
			})
			lastSyncTime = time.Now()
			return err
		}()
		if update.OnCompleteFunc != nil {
			update.OnCompleteFunc(err)
		}
		if err != nil {
			klog.Errorf(&quot;Error syncing pod %s (%q), skipping: %v&quot;, update.Pod.UID, format.Pod(update.Pod), err)
		}
		// 查看lastUndeliveredWorkUpdate是否有未处理的事件，继续处理
		p.wrapUp(update.Pod.UID, err)
	}
}
</code></pre><p>9  pkg/kubelet/kubelet.go:1395</p>
<p>syncPod: 在交由runtimeManager完成真正的处理逻辑之前，进行一些预处理</p>
<pre><code>func (kl *Kubelet) syncPod(o syncPodOptions) error {
	pod := o.pod
	mirrorPod := o.mirrorPod
	podStatus := o.podStatus
	updateType := o.updateType

	// 处理kill Pod
	if updateType == kubetypes.SyncPodKill {
		killPodOptions := o.killPodOptions
		if killPodOptions == nil || killPodOptions.PodStatusFunc == nil {
			return fmt.Errorf(&quot;kill pod options are required if update type is kill&quot;)
		}
		apiPodStatus := killPodOptions.PodStatusFunc(pod, podStatus)
		kl.statusManager.SetPodStatus(pod, apiPodStatus)
		// we kill the pod with the specified grace period since this is a termination
		if err := kl.killPod(pod, nil, podStatus, killPodOptions.PodTerminationGracePeriodSecondsOverride); err != nil {
			kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToKillPod, &quot;error killing pod: %v&quot;, err)
			// there was an error killing the pod, so we return that error directly
			utilruntime.HandleError(err)
			return err
		}
		return nil
	}
    ....
    // 是否能运行pod
	runnable := kl.canRunPod(pod)
	if !runnable.Admit {
		//不能回写container等待原因
		apiPodStatus.Reason = runnable.Reason
		apiPodStatus.Message = runnable.Message
		// Waiting containers are not creating.
		const waitingReason = &quot;Blocked&quot;
		for _, cs := range apiPodStatus.InitContainerStatuses {
			if cs.State.Waiting != nil {
				cs.State.Waiting.Reason = waitingReason
			}
		}
		for _, cs := range apiPodStatus.ContainerStatuses {
			if cs.State.Waiting != nil {
				cs.State.Waiting.Reason = waitingReason
			}
		}
	}

	// 更新statusManager pod状态
	kl.statusManager.SetPodStatus(pod, apiPodStatus)

	// 校验失败、标记删除、pod失败则kill pod
	if !runnable.Admit || pod.DeletionTimestamp != nil || apiPodStatus.Phase == v1.PodFailed {
		var syncErr error
		if err := kl.killPod(pod, nil, podStatus, nil); err != nil {
			kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToKillPod, &quot;error killing pod: %v&quot;, err)
			syncErr = fmt.Errorf(&quot;error killing pod: %v&quot;, err)
			utilruntime.HandleError(syncErr)
		} else {
			if !runnable.Admit {
				// There was no error killing the pod, but the pod cannot be run.
				// Return an error to signal that the sync loop should back off.
				syncErr = fmt.Errorf(&quot;pod cannot be run: %s&quot;, runnable.Message)
			}
		}
		return syncErr
	}

	// network plugin是否ready
	if err := kl.runtimeState.networkErrors(); err != nil &amp;&amp; !kubecontainer.IsHostNetworkPod(pod) {
		kl.recorder.Eventf(pod, v1.EventTypeWarning, events.NetworkNotReady, &quot;%s: %v&quot;, NetworkNotReadyErrorMsg, err)
		return fmt.Errorf(&quot;%s: %v&quot;, NetworkNotReadyErrorMsg, err)
	}

	// pod crgoup创建
	pcm := kl.containerManager.NewPodContainerManager()
	// 检查pod是否已经Terminate
	if !kl.podIsTerminated(pod) {
		// When the kubelet is restarted with the cgroups-per-qos
		// flag enabled, all the pod's running containers
		// should be killed intermittently and brought back up
		// under the qos cgroup hierarchy.
		// Check if this is the pod's first sync
		firstSync := true
		for _, containerStatus := range apiPodStatus.ContainerStatuses {
			if containerStatus.State.Running != nil {
				firstSync = false
				break
			}
		}
		// Don't kill containers in pod if pod's cgroups already
		// exists or the pod is running for the first time
		podKilled := false
		if !pcm.Exists(pod) &amp;&amp; !firstSync {
			if err := kl.killPod(pod, nil, podStatus, nil); err == nil {
				podKilled = true
			}
		}
		// Create and Update pod's Cgroups
		// Don't create cgroups for run once pod if it was killed above
		// The current policy is not to restart the run once pods when
		// the kubelet is restarted with the new flag as run once pods are
		// expected to run only once and if the kubelet is restarted then
		// they are not expected to run again.
		// We don't create and apply updates to cgroup if its a run once pod and was killed above
		if !(podKilled &amp;&amp; pod.Spec.RestartPolicy == v1.RestartPolicyNever) {
			if !pcm.Exists(pod) {
				if err := kl.containerManager.UpdateQOSCgroups(); err != nil {
					klog.V(2).Infof(&quot;Failed to update QoS cgroups while syncing pod: %v&quot;, err)
				}
				if err := pcm.EnsureExists(pod); err != nil {
					kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToCreatePodContainer, &quot;unable to ensure pod container exists: %v&quot;, err)
					return fmt.Errorf(&quot;failed to ensure that the pod: %v cgroups exist and are correctly applied: %v&quot;, pod.UID, err)
				}
			}
		}
	}

	// 如果是mirrorPod进入mirrorPod处理逻辑
	if kubetypes.IsStaticPod(pod) {
		podFullName := kubecontainer.GetPodFullName(pod)
		deleted := false
		if mirrorPod != nil {
			if mirrorPod.DeletionTimestamp != nil || !kl.podManager.IsMirrorPodOf(mirrorPod, pod) {
				// The mirror pod is semantically different from the static pod. Remove
				// it. The mirror pod will get recreated later.
				klog.Infof(&quot;Trying to delete pod %s %v&quot;, podFullName, mirrorPod.ObjectMeta.UID)
				var err error
				deleted, err = kl.podManager.DeleteMirrorPod(podFullName, &amp;mirrorPod.ObjectMeta.UID)
				if deleted {
					klog.Warningf(&quot;Deleted mirror pod %q because it is outdated&quot;, format.Pod(mirrorPod))
				} else if err != nil {
					klog.Errorf(&quot;Failed deleting mirror pod %q: %v&quot;, format.Pod(mirrorPod), err)
				}
			}
		}
		if mirrorPod == nil || deleted {
			node, err := kl.GetNode()
			if err != nil || node.DeletionTimestamp != nil {
				klog.V(4).Infof(&quot;No need to create a mirror pod, since node %q has been removed from the cluster&quot;, kl.nodeName)
			} else {
				klog.V(4).Infof(&quot;Creating a mirror pod for static pod %q&quot;, format.Pod(pod))
				if err := kl.podManager.CreateMirrorPod(pod); err != nil {
					klog.Errorf(&quot;Failed creating a mirror pod for %q: %v&quot;, format.Pod(pod), err)
				}
			}
		}
	}

	// 准备pod文件目录
	if err := kl.makePodDataDirs(pod); err != nil {
		kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToMakePodDataDirectories, &quot;error making pod data directories: %v&quot;, err)
		klog.Errorf(&quot;Unable to make pod data directories for pod %q: %v&quot;, format.Pod(pod), err)
		return err
	}

	// 等待挂载volumes
	if !kl.podIsTerminated(pod) {
		// Wait for volumes to attach/mount
		if err := kl.volumeManager.WaitForAttachAndMount(pod); err != nil {
			kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedMountVolume, &quot;Unable to attach or mount volumes: %v&quot;, err)
			klog.Errorf(&quot;Unable to attach or mount volumes for pod %q: %v; skipping pod&quot;, format.Pod(pod), err)
			return err
		}
	}

	// Fetch the pull secrets for the pod
	pullSecrets := kl.getPullSecretsForPod(pod)

	// 调用runtimeManager处理pod
	result := kl.containerRuntime.SyncPod(pod, podStatus, pullSecrets, kl.backOff)
	kl.reasonCache.Update(pod.UID, result)
	if err := result.Error(); err != nil {
		// Do not return error if the only failures were pods in backoff
		for _, r := range result.SyncResults {
			if r.Error != kubecontainer.ErrCrashLoopBackOff &amp;&amp; r.Error != images.ErrImagePullBackOff {
				// Do not record an event here, as we keep all event logging for sync pod failures
				// local to container runtime so we get better errors
				return err
			}
		}
		return nil
	}
	return nil
}
</code></pre><h3 id="参考文档">参考文档</h3>
<p><a href="https://juejin.cn/post/6844903694618607623">微软资深工程师详解 K8S 容器运行时</a></p>
<p><a href="https://www.cnblogs.com/luozhiyun/p/13736569.html">深入k8s：kubelet创建pod流程源码分析</a></p>

              
            </div>
          </div>
          <div id="post-footer" class="post-footer main-content-wrap">
            
              
            
            <div class="post-actions-wrap">
  
      <nav >
        <ul class="post-actions post-action-nav">
          
            <li class="post-action">
              
                <a class="post-action-btn btn btn--disabled">
              
                  <i class="fa fa-angle-left"></i>
                  <span class="hide-xs hide-sm text-small icon-ml">下一篇</span>
                </a>
            </li>
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="https://itech.red/2020/12/k8s-conformance-test%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/" data-tooltip="k8s conformance test问题记录">
              
                  <span class="hide-xs hide-sm text-small icon-mr">上一篇</span>
                  <i class="fa fa-angle-right"></i>
                </a>
            </li>
          
        </ul>
      </nav>
    <ul class="post-actions post-action-share" >
      
      
        <li class="post-action">
          <a class="post-action-btn btn btn--default" href="#disqus_thread">
            <i class="fa fa-comment-o"></i>
          </a>
        </li>
      
      <li class="post-action">
        
          <a class="post-action-btn btn btn--default" href="#">
        
          <i class="fa fa-list"></i>
        </a>
      </li>
    </ul>
  
</div>

            
              
                
<div id="lv-container" data-id="city" data-uid="MTAyMC80ODIxMS8yNDcwNw==">
<script type="text/javascript">
   (function(d, s) {
       var j, e = d.getElementsByTagName(s)[0];

       if (typeof LivereTower === 'function') { return; }

       j = d.createElement(s);
       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
       j.async = true;

       e.parentNode.insertBefore(j, e);
   })(document, 'script');
</script>
<noscript>为正常使用来必力评论功能请激活JavaScript</noscript>
</div>


              
            
          </div>
        </article>
        <footer id="footer" class="main-content-wrap">
  <span class="copyrights">
    &copy; 2021 <a href="https://github.com/ThinkMo">ThinkMo</a>. All Rights Reserved
  </span>
  <div class="busuanzi-count">
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  </div>
</footer>

      </div>
      <div id="bottom-bar" class="post-bottom-bar" data-behavior="5">
        <div class="post-actions-wrap">
  
      <nav >
        <ul class="post-actions post-action-nav">
          
            <li class="post-action">
              
                <a class="post-action-btn btn btn--disabled">
              
                  <i class="fa fa-angle-left"></i>
                  <span class="hide-xs hide-sm text-small icon-ml">下一篇</span>
                </a>
            </li>
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="https://itech.red/2020/12/k8s-conformance-test%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/" data-tooltip="k8s conformance test问题记录">
              
                  <span class="hide-xs hide-sm text-small icon-mr">上一篇</span>
                  <i class="fa fa-angle-right"></i>
                </a>
            </li>
          
        </ul>
      </nav>
    <ul class="post-actions post-action-share" >
      
      
        <li class="post-action">
          <a class="post-action-btn btn btn--default" href="#disqus_thread">
            <i class="fa fa-comment-o"></i>
          </a>
        </li>
      
      <li class="post-action">
        
          <a class="post-action-btn btn btn--default" href="#">
        
          <i class="fa fa-list"></i>
        </a>
      </li>
    </ul>
  
</div>

      </div>
      <div id="share-options-bar" class="share-options-bar" data-behavior="5">
  <i id="btn-close-shareoptions" class="fa fa-close"></i>
  <ul class="share-options">
    
  </ul>
</div>
<div id="share-options-mask" class="share-options-mask"></div>
    </div>
    
    <div id="about">
  <div id="about-card">
    <div id="about-btn-close">
      <i class="fa fa-remove"></i>
    </div>
    
      <img id="about-card-picture" src="https://avatars3.githubusercontent.com/u/6330242?v=3&amp;s=460" alt="作者的图片" />
    
    <h4 id="about-card-name">游</h4>
    
      <div id="about-card-bio">沉下心，多坚持</div>
    
    
      <div id="about-card-job">
        <i class="fa fa-briefcase"></i>
        <br/>
        SRE
      </div>
    
    
      <div id="about-card-location">
        <i class="fa fa-map-marker"></i>
        <br/>
        杭州
      </div>
    
  </div>
</div>

    

    
  
    <div id="cover" style="background-image:url('https://s2.ax1x.com/2020/01/09/lWSWLR.md.jpg');"></div>
  


    
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.7/js/jquery.fancybox.min.js" integrity="sha256-GEAnjcTqVP+vBp3SSc8bEDQqvWAZMiHyUSIorrWwH50=" crossorigin="anonymous"></script>


<script src="https://itech.red/js/script-pcw6v3xilnxydl1vddzazdverrnn9ctynvnxgwho987mfyqkuylcb1nlt.min.js"></script>


  
    <script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  

<script lang="javascript">
window.onload = updateMinWidth;
window.onresize = updateMinWidth;
document.getElementById("sidebar").addEventListener("transitionend", updateMinWidth);
function updateMinWidth() {
  var sidebar = document.getElementById("sidebar");
  var main = document.getElementById("main");
  main.style.minWidth = "";
  var w1 = getComputedStyle(main).getPropertyValue("min-width");
  var w2 = getComputedStyle(sidebar).getPropertyValue("width");
  var w3 = getComputedStyle(sidebar).getPropertyValue("left");
  main.style.minWidth = `calc(${w1} - ${w2} - ${w3})`;
}
</script>


  
    
      <script>
        var disqus_config = function () {
          this.page.url = 'https:\/\/itech.red\/2020\/12\/kubelet%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B%E4%B8%8Epod%E5%A4%84%E7%90%86%E4%B8%8A\/';
          
            this.page.identifier = 'k8s_kubelet_source';
          
        };
        (function() {
          
          
          if (window.location.hostname == "localhost") {
            return;
          }
          var d = document, s = d.createElement('script');
          var disqus_shortname = 'MTAyMC80ODIxMS8yNDcwNw==';
          s.src = '//' + disqus_shortname + '.disqus.com/embed.js';

          s.setAttribute('data-timestamp', +new Date());
          (d.head || d.body).appendChild(s);
        })();
      </script>
    
  




    
  </body>
</html>

