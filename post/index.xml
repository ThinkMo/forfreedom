<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 平凡世界</title>
    <link>https://itech.red/post/</link>
    <description>Recent content in Posts on 平凡世界</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 Feb 2021 10:13:30 +0800</lastBuildDate><atom:link href="https://itech.red/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>kube-scheduler源码解析</title>
      <link>https://itech.red/2021/02/kube-scheduler%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/</link>
      <pubDate>Tue, 23 Feb 2021 10:13:30 +0800</pubDate>
      
      <guid>https://itech.red/2021/02/kube-scheduler%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/</guid>
      <description>kube-scheduler设计 scheduler作为k8s的核心组件之一，其功能为每个Pod选择一个合适的node，其流程分为3步，首先获得未调度podList，然后通过一系列调度算法为Pod选择合适的node，将数据发送到apiserver。调度算法主要包含两部分predicates(预选) 和 priorities(优选)，最后将评分最高的node更新到Pod
For given pod: +---------------------------------------------+ | Schedulable nodes: | | | | +--------+ +--------+ +--------+ | | | node 1 | | node 2 | | node 3 | | | +--------+ +--------+ +--------+ | | | +-------------------+-------------------------+ | | v +-------------------+-------------------------+ Pred. filters: node 3 doesn&#39;t have enough resource +-------------------+-------------------------+ | | v +-------------------+-------------------------+ | remaining nodes: | | +--------+ +--------+ | | | node 1 | | node 2 | | | +--------+ +--------+ | | | +-------------------+-------------------------+ | | v +-------------------+-------------------------+ Priority function: node 1: p=2 node 2: p=5 +-------------------+-------------------------+ | | v select max{node priority} = node 2 scheduler支持的两种扩展方式</description>
    </item>
    
    <item>
      <title>kubelet源码解析之PLEG</title>
      <link>https://itech.red/2021/02/kubelet%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E4%B9%8Bpleg/</link>
      <pubDate>Tue, 09 Feb 2021 16:01:30 +0800</pubDate>
      
      <guid>https://itech.red/2021/02/kubelet%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E4%B9%8Bpleg/</guid>
      <description>kubelet源码分析之PLEG PLEG介绍 Pleg(Pod Lifecycle Event Generator) 是kubelet 的核心模块，PLEG周期性调用container runtime获取本节点containers/sandboxes的信息(像docker ps)，并与自身维护的podRecords信息进行对比，生成对应的 PodLifecycleEvent并发送到plegCh中，在kubelet syncLoop中对plegCh进行消费处理，最终达到用户的期望状态
在之前的kubelet版本中由每个pod的podWorker的goroutine来查询runtime进行对比，当pod数量逐渐增大时，大量周期性的查询导致高cpu使用率、低性能、难以扩展等问题，为了改进以上问题，引入了PLEG
创建 在/pkg/kubelet/kubelet.go:660 在函数KubeletNewMainKubelet中伴随Kubelet结构体创建
 // 传入runtime、chan缓冲大小、定时周期、podCache创建pleg klet.pleg = pleg.NewGenericPLEG(klet.containerRuntime, plegChannelCapacity, plegRelistPeriod, klet.podCache, clock.RealClock{}) pkg/kubelet/pleg/generic.go:109 工厂函数NewGenericPLEG创建返回接口类型PodLifecycleEventGenerator
// NewGenericPLEG instantiates a new GenericPLEG object and return it. func NewGenericPLEG(runtime kubecontainer.Runtime, channelCapacity int, relistPeriod time.Duration, cache kubecontainer.Cache, clock clock.Clock) PodLifecycleEventGenerator { return &amp;amp;GenericPLEG{ // 定时relist周期 relistPeriod: relistPeriod, // KubeGenericRuntimeManager runtime: runtime, // 与kubelet通信带缓冲chan eventChannel: make(chan *PodLifecycleEvent, channelCapacity), podRecords: make(podRecords), // podCache cache: cache, clock: clock, } } // podRecords定义 type podRecord struct { // 历史pod old *kubecontainer.</description>
    </item>
    
    <item>
      <title>kubelet源码解析-创建与删除POD(下)</title>
      <link>https://itech.red/2021/01/kubelet%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E5%88%9B%E5%BB%BA%E4%B8%8E%E5%88%A0%E9%99%A4pod%E4%B8%8B/</link>
      <pubDate>Fri, 08 Jan 2021 15:59:30 +0800</pubDate>
      
      <guid>https://itech.red/2021/01/kubelet%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E5%88%9B%E5%BB%BA%E4%B8%8E%E5%88%A0%E9%99%A4pod%E4%B8%8B/</guid>
      <description>kubeGenericRuntimeManager kubeGenericRuntimeManager为kubelet提供Runtime接口, 管理pods与containers生命周期, 调用remote runtime api完成操作；Kubelet与CRI交互如下图
SyncPod 接上文Pod处理走到调用kubeGenericRuntimeManager.SyncPod
pkg/kubelet/kuberuntime/kuberuntime_manager.go:661
func (m *kubeGenericRuntimeManager) SyncPod(pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, backOff *flowcontrol.Backoff) (result kubecontainer.PodSyncResult) { // 获得sandbox、container changes podContainerChanges := m.computePodActions(pod, podStatus) ...... // kill pod if podContainerChanges.KillPod { ....... // 先停止pod中所有conatiners，执行preStop，再停止sandbox killResult := m.killPodWithSyncResult(pod, kubecontainer.ConvertPodStatusToRunningPod(m.runtimeName, podStatus), nil) result.AddPodSyncResult(killResult) if killResult.Error() != nil { klog.Errorf(&amp;quot;killPodWithSyncResult failed: %v&amp;quot;, killResult.Error()) return } // 如果新建sandbox则清楚init containers if podContainerChanges.CreateSandbox { m.purgeInitContainers(pod, podStatus) } } else { // kill掉列表中所有containers，主要是处于未知状态的containers for containerID, containerInfo := range podContainerChanges.</description>
    </item>
    
    <item>
      <title>kubelet源码解析-启动流程与POD处理(上)</title>
      <link>https://itech.red/2020/12/kubelet%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B%E4%B8%8Epod%E5%A4%84%E7%90%86%E4%B8%8A/</link>
      <pubDate>Thu, 31 Dec 2020 17:13:30 +0800</pubDate>
      
      <guid>https://itech.red/2020/12/kubelet%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B%E4%B8%8Epod%E5%A4%84%E7%90%86%E4%B8%8A/</guid>
      <description>kubelet介绍 在k8s集群中的每个节点上都运行着一个kubelet服务进程，其主要负责向apiserver注册节点、管理pod及pod中的容器，并通过 cAdvisor 监控节点和容器的资源。
 节点管理：节点注册、节点状态更新(定期心跳) pod管理：接受来自apiserver、file、http等PodSpec，并确保这些 PodSpec 中描述的容器处于运行状态且运行状况良好 容器健康检查：通过ReadinessProbe、LivenessProbe两种探针检查容器健康状态 资源监控：通过 cAdvisor 获取其所在节点及容器的监控数据  kubelet组件模块 kubelet组件模块如上图所示，下面先对几个比较重要的几个模块进行简单说明：
 Pleg(Pod Lifecycle Event Generator) 是kubelet 的核心模块，PLEG周期性调用container runtime获取本节点containers/sandboxes的信息(像docker ps)，并与自身维护的pods cache信息进行对比，生成对应的 PodLifecycleEvent并发送到plegCh中，在kubelet syncLoop中对plegCh进行消费处理，最终达到用户的期望状态，相关设计可以看这里 podManager提供存储、访问Pod信息的接口，维护static pod和mirror pod的映射关系 containerManager 管理容器的各种资源，比如 CGroups、QoS、cpuset、device 等 KubeletGenericRuntimeManager是容器运行时的管理者，负责于 CRI 交互，完成容器和镜像的管理；关于client/server container runtime 设计可以看这里 statusManager负责维护pod状态信息并负责同步到apiserver probeManager负责探测pod状态，依赖statusManager、statusManager、livenessManager、startupManager cAdvisor是google开源的容器监控工具，集成在kubelet中，收集节点与容器的监控信息，对外提供查询接口 volumeManager 管理容器的存储卷，比如格式化资盘、挂载到 Node 本地、最后再将挂载路径传给容器  深入kubelet工作原理 从上图可以看出kubelet的主要工作逻辑在SyncLoop控制循环中，处理pod更新事件、pod生命周期变化、周期sync事件、定时清理事件；SyncLoop旁还有处理其他逻辑的控制循环，例如处理pod状态同步的statusManager，处理健康检查的probeManager等。
下面将从kubelet源码来分析各个环节的处理逻辑。
kubelet代码(版本1.19)主要位于cmd/kubelet与pkg/kubelet下，首先看下kubelet启动流程与初始化
1 cmd/kubelet/kubelet.go:34
func main() { rand.Seed(time.Now().UnixNano()) command := app.NewKubeletCommand() logs.InitLogs() defer logs.FlushLogs() if err := command.Execute(); err !</description>
    </item>
    
    <item>
      <title>k8s conformance test问题记录</title>
      <link>https://itech.red/2020/12/k8s-conformance-test%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/</link>
      <pubDate>Fri, 25 Dec 2020 17:03:30 +0800</pubDate>
      
      <guid>https://itech.red/2020/12/k8s-conformance-test%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/</guid>
      <description>K8S conformance test问题记录 K8S conformance test步骤与提交可以参考这里, 本文主要记录相关问题
私有云环境测试镜像准备 在隔离的私有云环境跑测试，需要将K8S conformance test需要的镜像提前准备好并存储到私有镜像仓库中
1. 获取所有测试镜像 sonobuoy images --kubernetes-version v1.19.3 &amp;gt; images #我这里是v1.19.3版本k8s，可以替换 2. 在非离线环境中下载镜像包 for i in `cat images`; do docker pull $i ;done docker save `cat image | tr &amp;quot;\n&amp;quot; &amp;quot; &amp;quot;` -o images.tar 3. 在离线环境加载镜像包 docker load -i images.tar 4. 生成e2e repo配置，并将所有配置都替换为私有镜像仓库地址 sonobuoy gen default-image-config &amp;gt; repo.yaml 5. push镜像 sonobuoy images push --e2e-repo-config repo.yaml 使用私有镜像仓库测试 开始测试 registry_url=xx sonobuoy run --mode=certified-conformance --sonobuoy-image $registry_url/sonobuoy:v0.</description>
    </item>
    
    <item>
      <title>博客theme修改记录</title>
      <link>https://itech.red/2020/12/%E5%8D%9A%E5%AE%A2theme%E4%BF%AE%E6%94%B9%E8%AE%B0%E5%BD%95/</link>
      <pubDate>Sun, 13 Dec 2020 21:40:30 +0800</pubDate>
      
      <guid>https://itech.red/2020/12/%E5%8D%9A%E5%AE%A2theme%E4%BF%AE%E6%94%B9%E8%AE%B0%E5%BD%95/</guid>
      <description>blog theme修改记录 之前blog theme相关修改的代码丢失了， 需要重新配置一下，顺手记录一下
博客由hugo生成，关于hugo看这里
hugo主题使用hugo-tranquilpeak-theme
评论设置 hugo-tranquilpeak-theme 仅支持disqus，但对于国内来说无法访问，这里修改了相关代码，使用来必力，注册来必力，选择免费的City版本安装
删除默认的 layouts/partials/post/disqus.html 的代码，将安装代码注入即可
layouts/partials/post/disqus.html &amp;lt;!-- 来必力City版安装代码 --&amp;gt; &amp;lt;div id=&amp;quot;lv-container&amp;quot; data-id=&amp;quot;city&amp;quot; data-uid=&amp;quot;你的uid&amp;quot;&amp;gt; &amp;lt;script type=&amp;quot;text/javascript&amp;quot;&amp;gt; (function(d, s) { var j, e = d.getElementsByTagName(s)[0]; if (typeof LivereTower === &#39;function&#39;) { return; } j = d.createElement(s); j.src = &#39;https://cdn-city.livere.com/js/embed.dist.js&#39;; j.async = true; e.parentNode.insertBefore(j, e); })(document, &#39;script&#39;); &amp;lt;/script&amp;gt; &amp;lt;noscript&amp;gt;为正常使用来必力评论功能请激活JavaScript&amp;lt;/noscript&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;!-- City版安装代码已完成 --&amp;gt; 浏览统计 浏览统计使用的是不蒜子
修改hugo配置文件config.toml在[params]下添加相关js
 [[params.customJS]] src = &amp;quot;//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js&amp;quot; 修改layouts/partials/footer.html在copyright下添加
 &amp;lt;div class=&amp;quot;busuanzi-count&amp;quot;&amp;gt; &amp;lt;span class=&amp;quot;site-uv&amp;quot;&amp;gt; &amp;lt;i class=&amp;quot;fa fa-user&amp;quot;&amp;gt;&amp;lt;/i&amp;gt; &amp;lt;span class=&amp;quot;busuanzi-value&amp;quot; id=&amp;quot;busuanzi_value_site_uv&amp;quot;&amp;gt;&amp;lt;/span&amp;gt; &amp;lt;/span&amp;gt; &amp;lt;span class=&amp;quot;site-pv&amp;quot;&amp;gt; &amp;lt;i class=&amp;quot;fa fa-eye&amp;quot;&amp;gt;&amp;lt;/i&amp;gt; &amp;lt;span class=&amp;quot;busuanzi-value&amp;quot; id=&amp;quot;busuanzi_value_site_pv&amp;quot;&amp;gt;&amp;lt;/span&amp;gt; &amp;lt;/span&amp;gt; &amp;lt;/div&amp;gt; </description>
    </item>
    
    <item>
      <title>k8s 1.17.5高可用部署</title>
      <link>https://itech.red/2020/07/k8s-1.17.5%E9%AB%98%E5%8F%AF%E7%94%A8%E9%83%A8%E7%BD%B2/</link>
      <pubDate>Thu, 23 Jul 2020 17:03:30 +0800</pubDate>
      
      <guid>https://itech.red/2020/07/k8s-1.17.5%E9%AB%98%E5%8F%AF%E7%94%A8%E9%83%A8%E7%BD%B2/</guid>
      <description>k8s 1.17.5高可用部署 环境准备 K8S版本: 1.17.5
   IP 操作系统 主机配置 备注     192.168.0.2 debian9.5 4核8G control plane(apiserver/scheduler/controller/etcd)   192.168.0.3 debian9.5 4核8G control plane(apiserver/scheduler/controller/etcd)   192.168.0.4 debian9.5 4核8G control plane(apiserver/scheduler/controller/etcd)   192.168.0.5 debian9.5 4核8G node   192.168.0.6 debian9.5 4核8G node   192.168.0.7 debian9.5 4核8G node   59.111.230.7 LB X 负载均衡, 后端为控制节点(可换为keepalived VIP)    机器初始化(所有节点操作)  使用aliyun debian源 部署docker 18.09.9 (https://docs.</description>
    </item>
    
    <item>
      <title>构建ElasticSearch集群之前</title>
      <link>https://itech.red/2018/12/%E6%9E%84%E5%BB%BAelasticsearch%E9%9B%86%E7%BE%A4%E4%B9%8B%E5%89%8D/</link>
      <pubDate>Wed, 12 Dec 2018 17:03:30 +0800</pubDate>
      
      <guid>https://itech.red/2018/12/%E6%9E%84%E5%BB%BAelasticsearch%E9%9B%86%E7%BE%A4%E4%B9%8B%E5%89%8D/</guid>
      <description>摘自designing the perfect elasticsearch cluster 
 本文包含了设计elasticsearch cluster需要了解的方方面面，本文不会告诉你如何设计一个完美的es集群。
  elasticsearch是一个搜索引擎，不是数据库，不会替代mysql
  elasticsearch是一个弹性搜索引擎，弹性体现在水平扩展、索引分片；elastic将索引拆分成多个分片存储在不同的host上，分片数量默认为5，需要根据实际workload调整以达到最优性能
  高可用设计
  elasticsearch集群包含一下4种节点类型： master nodes: 主节点 data node: 数据节点 http node: 查询节点 coordinating node: 协作节点 最小高可用集群设计： 1. 3数据中心 2. 3主节点，奇数个主节点防止脑裂，将3个主机点分散在各个数据中心 3. 2查询节点，在主节点数据中心各一个 4. 任意数量的数据节点 ES的分片分配算法允许分片分配在不同区域内，根据rack配置将主分片、备份分片分配在不同的区域中 cluster.routing.allocation.awareness.attributes: &amp;quot;rack_id&amp;quot; node.attr.rack_id: &amp;quot;dontsmokecrack&amp;quot;  理解luence: elasticsearch使用了luence库  1. 每个elasticsearch分片是一个luence索引，一个luence索引最大可包含2,147,483,519条记录。 2. luence将索引划分为多个段，luence顺序搜索这些段；当新写请求来时会创建段，写提交或关闭时， 段即不可变，有新记录添加到elasticsearch index时，luence会创建新的段，luence会不断的将小的段 合并成大的段。luence搜索段是串行的，所以段越多延迟越大。当luence合并时会占用CPU与I/O进而降 低index速度。 3. 当更新或删除文档时luence执行copy on write操作，删除只将文档标记为已删除，索引磁盘占用会 一直增长，除非整个索引删除。 4. luence执行merge时，会将两个段合并成一个新的段后再删除旧的段，merge要确保磁盘空间足够  硬件  CPU: 复杂查询 ElasticSearch主要通过thread_pools使用CPU资源，主要的thread_pool有generic、index、get、bulk、 search等，可用通过GET _nodes/thread_pool?</description>
    </item>
    
    <item>
      <title>Python GIL与线程安全</title>
      <link>https://itech.red/2018/08/python-gil%E4%B8%8E%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8/</link>
      <pubDate>Fri, 10 Aug 2018 15:17:59 +0800</pubDate>
      
      <guid>https://itech.red/2018/08/python-gil%E4%B8%8E%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8/</guid>
      <description>python GIL与多线程  GIL: Global Interpreter Lock(全局解释器锁)，python虚拟机一级的互斥机制，用来保护资源的互斥访问，尤其是python对象的引用计数。 python代码执行  获得GIL 执行代码直到被阻塞或N条指令执行之后(py3:超时时间)被python 虚拟机调度 释放GIL 何时挂起当前线程：被阻塞或N条指令执行之后(py3:超时时间15ms) 如何选择等待的线程进行调度：由底层操作系统决定  python多线程机制是在GIL基础上实现的 python的多线程机制建立在操作系统的原生线程的基础之上  Thread safe in python Python线程安全与传统的java、c有很大不同，python的很多操作都是原子的，例如sort函数本身是原子操作，不会被其他线程看到排序一半的list；而有一些操作不是原子的，例如+=
n=0 def add(): global n n += 1 import dis #将python byte code转为助记符号 dis.dis(add) 3 0 LOAD_GLOBAL 0 (n) 3 LOAD_CONST 1 (1) 6 INPLACE_ADD 7 STORE_GLOBAL 0 (n) 10 LOAD_CONST 0 (None) 13 RETURN_VALUE 操作步骤为：
 加载n到栈 加载常量n到栈 将两者相加 将结果付给n  由于操作不是原子的，python解释器可能在某一步被抢占，造成不一致的情况，执行以下代码，将不一定会得到100
threads = [] for i in range(100): t = threading.</description>
    </item>
    
    <item>
      <title>ElasticSearch配置滚动更新</title>
      <link>https://itech.red/2018/08/elasticsearch%E9%85%8D%E7%BD%AE%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0/</link>
      <pubDate>Mon, 06 Aug 2018 19:47:59 +0800</pubDate>
      
      <guid>https://itech.red/2018/08/elasticsearch%E9%85%8D%E7%BD%AE%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0/</guid>
      <description>起因 最近Logstash经常打印如下日志内容，虽然只是INFO级别的错误但还是引起了我的注意，大概意思是ES的bulk thread pool大小为32已经满负荷，队列也满了，所以拒绝了Logstash的bulk请求
[2018-06-23T08:00:47,281][INFO ][logstash.outputs.elasticsearch] retrying failed action with response code: 429 ({&amp;quot;type&amp;quot;=&amp;gt;&amp;quot;es_rejected_execution_exception&amp;quot;, &amp;quot;reason&amp;quot;=&amp;gt;&amp;quot;rejected execution of org.elasticsearch.transport.TransportService$7@177d229b on EsThreadPoolExecutor[bulk, queue capacity = 1000, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@66a1a8c3[Running, pool size = 32, active threads = 32, queued tasks = 1019, completed tasks = 832616617]]&amp;quot;}) 解决思路  扩大处理bulk的thread pool线程数量  当前使用的服务器CPU核数为176(cat /proc/cpuinfo)，而在ES源码中为了避免引起Java oom线程数会取min(32, 核数)，所以造成默认的thread_pool.bulk.size为32，为了扩大线程容量需在ES中添加以下配置 processors: 96 thread_pool.bulk.size: 96  增加等待队列长度  thread_pool.bulk.queue_size: 3000 更新配置 这些配置不能通过ES Setting API来更改，只能通过滚动重启的方式，下面简单记录下过程
 禁止分配分片  curl -X PUT &amp;quot;ES_HOST:9200/_cluster/settings&amp;quot; -H &#39;Content-Type: application/json&#39; -d&#39; { &amp;quot;transient&amp;quot;: { &amp;quot;cluster.</description>
    </item>
    
    <item>
      <title>ELK集群2.x到5.6升级记录</title>
      <link>https://itech.red/2018/04/elk%E9%9B%86%E7%BE%A42.x%E5%88%B05.6%E5%8D%87%E7%BA%A7%E8%AE%B0%E5%BD%95/</link>
      <pubDate>Mon, 02 Apr 2018 20:10:59 +0800</pubDate>
      
      <guid>https://itech.red/2018/04/elk%E9%9B%86%E7%BE%A42.x%E5%88%B05.6%E5%8D%87%E7%BA%A7%E8%AE%B0%E5%BD%95/</guid>
      <description>考虑到目前使用的ELK集群版本与开源版本的版本差距有点大，而ELK5.6相较2.3版本性能有较大提升，尤其是Logstash grok插件，最近对测试环境的两个ELK集群进行了升级，对升级过程进行一个记录；升级主要参考了官方文档
当前系统运行版本  filebeat：1.2.3 kibana：4.5.1 logstash：2.3.2 elasticsearch：2.3.3 JVM: 1.7  升级顺序  jvm(略) elasticsearch kibana logstash beats  升级Elasticsearch 升级之前  升级前使用Elasticsearch Migration Plugin查看潜在问题 最好先在测试环境升级ELK 在升级之前先备份数据  升级（跨大版本整个集群需重启）   禁止自动分配分片，在停止elasticsearch时减少不必要的I/O
curl -XPUT &#39;localhost:9200/_cluster/settings?pretty&#39; -H &#39;Content-Type: application/json&#39; -d&#39;{ &amp;quot;persistent&amp;quot;: {&amp;quot;cluster.routing.allocation.enable&amp;quot;: &amp;quot;none&amp;quot;} }&#39;   同步刷新以加快分片恢复
curl -XPOST &#39;localhost:9200/_flush/synced?pretty&#39;  停止并升级各个节点（安装新的rpm或deb，更新配置文件，配置文件有较大改动）
/etc/init.d/elasticsearch stop apt-get pruge elasticsearch(所有deb包已在本地apt源中,删除原配置文件) apt-get install elasticsearch=5.6.5   升级插件（需删除旧的插件）
/usr/share/elasticsearch/bin/elasticsearch-plubin remove license /usr/share/elasticsearch/bin/elasticsearch-plubin remove marvel /usr/share/elasticsearch/bin/elasticsearch-plubin install x-pack   启动集群各个节点，保证所有节点均加入集群(优先启动master node)</description>
    </item>
    
    <item>
      <title>ElasticSearch索引类型映射</title>
      <link>https://itech.red/2018/01/elasticsearch%E7%B4%A2%E5%BC%95%E7%B1%BB%E5%9E%8B%E6%98%A0%E5%B0%84/</link>
      <pubDate>Fri, 19 Jan 2018 18:10:59 +0800</pubDate>
      
      <guid>https://itech.red/2018/01/elasticsearch%E7%B4%A2%E5%BC%95%E7%B1%BB%E5%9E%8B%E6%98%A0%E5%B0%84/</guid>
      <description>最近接手维护了几个ELK集群，对接的是IaaS、PaaS服务日志，简单的ELK架构，通过filebeat采集日志，发送到logstash结构化日志然后发送到ElasticSearch，用户可以通过Kibana查看服务日志定位问题、做一些性能分析图表等，同时利用elastalert做日志报警。
在ElasticSearch须要对用户的索引建立合适的类型映射(尤其是int类型)，才可以在kibana中对数据进行分析，关于ES的映射类型可以看这里，需指出的是一旦某个field字段类型确定就很难更改该字段的类型(需reindex)。
Logstash将非结构化数据转化为结构化数据，通过JSON将数据发送给ElasticSearch，所有字段都会被当作string来处理，而ElasticSearch在自动判断字段类型建立映射这方面做的不足，与我们需求不符，那么如何正确建立索引类型映射呢？
一、通过Logstash的grok、mutate确定字段类型
  grok
根据grok官方文档，在grok正则表达式后可以添加一个数据类型，默认是string类型，如果你想使字段类型为int，你可以在表达式后加int，例如%{NUMBER:num:int}，那么num字段会从string类型变为int类型，目前只支持int和float。
  mutate
通过mutate可以将field转化为integer、float、string，例如：
filter { mutate { convert =&amp;gt; { &amp;quot;num&amp;quot; =&amp;gt; &amp;quot;integer&amp;quot; } } }   二、ElasticSearch mapping template
映射(mappings)决定了一个字段(field)如何被ElasticSearch解释、存储，例如数据{&amp;ldquo;ip&amp;rdquo;:&amp;ldquo;223.5.5.5&amp;rdquo;}发送给ES，ES会将ip字段存储为string类型，而不是ip类型，不能做IP范围查询同时造成存储空间浪费、查询效率低等。**不管在Logstash如何转换类型，ElasticSearch不会知道你的用意除非你正确映射。**所有整型会存为long，小数会存为float或double，关于最小类型可以看这里，使用integer而不是long会有效的减少ELasticSearch负担。
  映射模版
编写模版文件my_template.json
{ &amp;quot;index_patterns&amp;quot; : &amp;quot;index*&amp;quot;, &amp;quot;version&amp;quot; : 1, &amp;quot;settings&amp;quot; : { &amp;quot;index.refresh_interval&amp;quot; : &amp;quot;5s&amp;quot; }, &amp;quot;mappings&amp;quot; : { &amp;quot;_default_&amp;quot; : { &amp;quot;properties&amp;quot;:{ &amp;quot;host&amp;quot;: { &amp;quot;type&amp;quot; : &amp;quot;string&amp;quot;}, &amp;quot;ip&amp;quot;: {&amp;quot;type&amp;quot; : &amp;quot;ip&amp;quot;}, .... } } } }   上传template.</description>
    </item>
    
    <item>
      <title>Logstash过滤插件grok正则解析</title>
      <link>https://itech.red/2017/10/logstash%E8%BF%87%E6%BB%A4%E6%8F%92%E4%BB%B6grok%E6%AD%A3%E5%88%99%E8%A7%A3%E6%9E%90/</link>
      <pubDate>Tue, 10 Oct 2017 21:31:09 +0800</pubDate>
      
      <guid>https://itech.red/2017/10/logstash%E8%BF%87%E6%BB%A4%E6%8F%92%E4%BB%B6grok%E6%AD%A3%E5%88%99%E8%A7%A3%E6%9E%90/</guid>
      <description>Logstash过滤插件grok正则解析 一、grok介绍 grok是Logstash中用来解析非结构化日志数据，将日志转化为可查询的结构化数据的最佳方法，可以用来处理syslog日志、apache等webserver日志、mysql日志以及用户自定义日志。
Logstash自带有120多种预定义好的正则表达式方便用户使用，你可以在这里查看这些正则表达式，你也可以添加自己的匹配规则。
有两个网站可以帮助我们来构建正则表达式去匹配我们的日志：
 http://grokdebug.herokuapp.com http://grokconstructor.appspot.com (推荐)  二、grok基础 grok匹配模式语法为：%{SYNTAX:SEMANTIC:TYPE}
SYNTAX: 正则表达式、预定义的正则表达式名称
SEMANTIC: 标识符，标识匹配后的数据
TYPE: 可选的类型，目前支持int、float
例如：NUMBER可以匹配3.44，IP可以匹配：192.168.21.2
一个复杂的日志格式如下：
192.168.21.2 GET /index.html 15823 0.023 grok匹配模式可以为：
${IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration} 更加实际的，该条日志可能来自一个文件：
input { file { path =&amp;gt; &amp;quot;/var/log/http.log&amp;quot; } } filter { grok { match =&amp;gt; { &amp;quot;message&amp;quot; =&amp;gt; &amp;quot;%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}&amp;quot; } } } 在grok过滤后，可以得到额外一下字段：
client: 192.168.21.2 method: GET request: /index.html bytes: 15823 duration: 0.023 grok中的正则表达式 Grok采用了Oniguruma正则表达式库，在on the Oniguruma site看到支持的正则语法。</description>
    </item>
    
    <item>
      <title>Ext2设计与实现(译)</title>
      <link>https://itech.red/2017/04/ext2%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E8%AF%91/</link>
      <pubDate>Thu, 06 Apr 2017 19:10:59 +0800</pubDate>
      
      <guid>https://itech.red/2017/04/ext2%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E8%AF%91/</guid>
      <description>Ext2设计与实现 1、介绍 Linux第一版只支持Minix文件系统，Minix文件系统有两大亟待解决的限制：块地址存储在16位整型，最大文件大小为64MB；另外目录和文件名长度最大支持14个字符。
我们设计与实现了两种新的文件系统:EXT 、 EXT2
在这篇论文中，我们将简述Linux文件系统的历史，简单介绍Unix文件系统的基础概念；介绍Linux VFS层的实现并详细介绍EXT2内核代码与用户工具；最后，是Linux、BSD下EXT2性能测试对比。
2、Linux文件系统历史（略） 3、文件系统概念 Linux文件系统的实现基于Unix操作系统通用概念：文件通过inodes来表示，目录是一种简单的文件包含许多列表项，设备是可以发起I/O请求的特殊文件。
3.1 Inode 每个文件都使用inode结构体来表示，每个inode包含了描述文件元数据：文件类型、访问权限、所有者、时间戳、大小、数据块指针。分配给文件的数据块地址存储在文件的inode节点中，当用户对文件发起I/O请求时，内核代码将当前文件偏移量转为块号，使用该数字作为块地址表的索引来读写物理块，如下图所示：
3.2 目录 目录按层次树结构组织，每一个目录可以包含文件和子目录。
目录实现为一种特殊的文件。事实上，目录是一种包含列表项的文件，每一项包含一个inode号和一个文件名，当一个进程使用一个路径名时，内核代码会搜索目录查找对应的inode号，在路径文件名被转换为inode号后，inode结构会被存储到内存中用以后序请求。目录如下图：
3.3 链接 Unix文件系统提出了链接的概念，若干个文件名可以关联到一个inode节点，inode节点包含一个存储链接数的域。增加一个链接会创建一个目录项并增加inode链接计数。当删除链接时，内核会递减链接计数，为0时删除inode。
这种类型的链接称为硬链接，只能在一个文件系统中使用，且不能链接到一个目录，避免引起环路。
另一种链接存在于大多数Unix系操作系统，符号链接：只包含文件名的简单文件，当内核inode转换时遇到符号链接，会将软链接文件内容替换链接名。因为软链接不包含inode，它可以跨文件系统，可以指向任何类型的文件，甚至不存在的文件。但软链接会占用磁盘空间、inode、在路径名到inode转换时引起额外消耗。
3.4 设备文件 在Unix系操作系统，设备被当作特殊文件访问。一个设备文件并不占用文件系统空间，只作为设备驱动访问接口。
有两类特殊文件：字符设备、块设备。主设备号决定类型，次设备号决定哪一个设备。
4 VFS VFS是一个文件系统抽象层，定义了一个文件系统应该实现的操作，对上层屏蔽了底层不同文件系统的实现，一图概之：
5 EXT2 5.1 起因 修复EXT文件系统问题，提供一个强大的文件系统，实现unix文件语义并提供高级特性
5.2 标准 ext2fs特性  支持标准Unix文件类型：普通文件、目录、设备文件、符号链接 支持最大4TB文件系统 长文件名：255字节，可扩展至1012 为root保留空间以便修复  5.3 高级 ext2fs特性  属性继承 软链接：目标名存储在inode中 创建文件系统时可选择逻辑块大小 fsck mount options Append-only files  5.4 物理结构 受BSD文件系统的影响，文件系统由块组构成，但块组并没有与磁盘的物理结构块绑定，因为现代驱动趋势是优化顺序访问和对操作系统隐藏物理结构。
文件系统物理结构：
每一个块组包含一份冗余的文件系统控制信息（超级块和文件系统描述信息），并包含一部分文件系统（块位图、inode位图、inode表、数据块）：
使用block group提高系统可靠性，冗余超级块信息可以简化文件系统恢复；inode表与数据块一起存储提高寻道时间，提高性能。
在ext2fs中，目录使用链表管理变长目录项，每一项包含inode号、记录项长度、文件名和文件名长度。通过变长目录项可以实现长文件名同时减少磁盘空间浪费，目录项结构如下：
5.5 性能优化  预读：当读取一个块时，内核会请求连续的若干块，当读取下一个块时会从buffer cache读取 分配优化： 块组的inode、data分配会在当前组中以减少寻道时间 写时预分配  6 Ext2fs library （略） 译自：http://e2fsprogs.</description>
    </item>
    
    <item>
      <title>springmvc实现导出数据excel</title>
      <link>https://itech.red/2016/11/springmvc%E5%AE%9E%E7%8E%B0%E5%AF%BC%E5%87%BA%E6%95%B0%E6%8D%AEexcel/</link>
      <pubDate>Wed, 23 Nov 2016 18:47:34 +0800</pubDate>
      
      <guid>https://itech.red/2016/11/springmvc%E5%AE%9E%E7%8E%B0%E5%AF%BC%E5%87%BA%E6%95%B0%E6%8D%AEexcel/</guid>
      <description>springmvc实现导出数据excel
最近在项目中要实现将数据导出为excel的功能，研究了下目前springmvc框架下excel导出的方式，在spring 4.3中使用AbstractXlsView来实现，AbstractExcelView已被弃用；但这里介绍的是直接用apache poi实现的一种方式。
 model如下  package red.itech.blog.dao.model; import java.util.Date; /** * Created by you on 16/10/28. */ public class Blog { private String author; private Date createdAt; private String title; private long count; public Blog(){} public Blog(String author, Date createdAt, String title, long count){ this.author = author; this.count = count; this.createdAt = createdAt; this.title =title; } public void setAuthor(String author) { this.author = author; } public void setCreatedAt(Date createdAt) { this.</description>
    </item>
    
    <item>
      <title>Spring＋Mybatis＋Velocity项目搭建</title>
      <link>https://itech.red/2016/09/springmybatisvelocity%E9%A1%B9%E7%9B%AE%E6%90%AD%E5%BB%BA/</link>
      <pubDate>Wed, 28 Sep 2016 15:43:54 +0800</pubDate>
      
      <guid>https://itech.red/2016/09/springmybatisvelocity%E9%A1%B9%E7%9B%AE%E6%90%AD%E5%BB%BA/</guid>
      <description>Spring＋Mybatis＋Velocity项目搭建
一、开发工具
 JDK 1.8.0_91 Intellij IDEA 15.0.6 Mysql 5.5.44 Maven 3  二、新建工程
  新建Maven工程，不选Create from archtype提供的工程模版，为了学习从头开始配置工程，点击next设置工程坐标，然后一路到finish。  &amp;lt;groupId&amp;gt;red.itech&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;blogDemo&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.0-SNAPSHOT&amp;lt;/version&amp;gt;    如果使用git开发，可以新建.gitignore文件，使git忽略idea自动生成的文件（.ignore插件可以帮助生成.gitignore），项目git初始化
# Created by .ignore support plugin (hsz.mobi) .gitignore ### OSX template *.DS_Store # IntelliJ project files .idea *.iml out target gen### Java template *.class # Package Files # *.jar *.war *.ear ` 三、Spring MVC
  编辑pom.xml添加Spring MVC、servlet依赖如下：
 &amp;lt;dependencies&amp;gt; &amp;lt;!-- spring --&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.</description>
    </item>
    
    <item>
      <title>理解UML类图关系</title>
      <link>https://itech.red/2016/08/%E7%90%86%E8%A7%A3uml%E7%B1%BB%E5%9B%BE%E5%85%B3%E7%B3%BB/</link>
      <pubDate>Fri, 26 Aug 2016 16:51:34 +0800</pubDate>
      
      <guid>https://itech.red/2016/08/%E7%90%86%E8%A7%A3uml%E7%B1%BB%E5%9B%BE%E5%85%B3%E7%B3%BB/</guid>
      <description>理解UML类图关系
1、依赖(Dependency)
  关系：uses temporarily，使用关系，作为局部变量、方法参数或者对静态方法的调用
  代码示例：
 import B; public class A{ public void method1(B b) { // ... } public void method2() { B tempB = new B(); // ... } }    图示(一套带箭头的虚线表示)：
  2、聚合(Aggregation)
  关系：is part of，整体与部分的关系，作为成员变量
  代码：
 import Engine; public class Car{ private Engine engine; publilc Engine getEngine(){ return engine; } }    图示(一条带空心菱形箭头的直线表示)：</description>
    </item>
    
  </channel>
</rss>
