<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>平凡世界</title>
    <link>http://www.itech.red/index.xml</link>
    <description>Recent content on 平凡世界</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 06 Apr 2017 19:10:59 +0800</lastBuildDate>
    <atom:link href="http://www.itech.red/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Ext2设计与实现(译)</title>
      <link>http://www.itech.red/2017/04/ext2%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E8%AF%91/</link>
      <pubDate>Thu, 06 Apr 2017 19:10:59 +0800</pubDate>
      
      <guid>http://www.itech.red/2017/04/ext2%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E8%AF%91/</guid>
      <description>

&lt;h2 id=&#34;ext2设计与实现&#34;&gt;Ext2设计与实现&lt;/h2&gt;

&lt;h3 id=&#34;1-介绍&#34;&gt;1、介绍&lt;/h3&gt;

&lt;p&gt;Linux第一版只支持Minix文件系统，Minix文件系统有两大亟待解决的限制：块地址存储在16位整型，最大文件大小为64MB；另外目录和文件名长度最大支持14个字符。&lt;/p&gt;

&lt;p&gt;我们设计与实现了两种新的文件系统:EXT 、 EXT2&lt;/p&gt;

&lt;p&gt;在这篇论文中，我们将简述Linux文件系统的历史，简单介绍Unix文件系统的基础概念；介绍Linux VFS层的实现并详细介绍EXT2内核代码与用户工具；最后，是Linux、BSD下EXT2性能测试对比。&lt;/p&gt;

&lt;h3 id=&#34;2-linux文件系统历史-略&#34;&gt;2、Linux文件系统历史（略）&lt;/h3&gt;

&lt;h3 id=&#34;3-文件系统概念&#34;&gt;3、文件系统概念&lt;/h3&gt;

&lt;p&gt;Linux文件系统的实现基于Unix操作系统通用概念：文件通过inodes来表示，目录是一种简单的文件包含许多列表项，设备是可以发起I/O请求的特殊文件。&lt;/p&gt;

&lt;h4 id=&#34;3-1-inode&#34;&gt;3.1 Inode&lt;/h4&gt;

&lt;p&gt;每个文件都使用inode结构体来表示，每个inode包含了描述文件元数据：文件类型、访问权限、所有者、时间戳、大小、数据块指针。分配给文件的数据块地址存储在文件的inode节点中，当用户对文件发起I/O请求时，内核代码将当前文件偏移量转为块号，使用该数字作为块地址表的索引来读写物理块，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://occ9e8dkg.bkt.clouddn.com/ext2-inode.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;3-2-目录&#34;&gt;3.2 目录&lt;/h4&gt;

&lt;p&gt;目录按层次树结构组织，每一个目录可以包含文件和子目录。&lt;/p&gt;

&lt;p&gt;目录实现为一种特殊的文件。事实上，目录是一种包含列表项的文件，每一项包含一个inode号和一个文件名，当一个进程使用一个路径名时，内核代码会搜索目录查找对应的inode号，在路径文件名被转换为inode号后，inode结构会被存储到内存中用以后序请求。目录如下图：&lt;/p&gt;

&lt;h4 id=&#34;3-3-链接&#34;&gt;3.3 链接&lt;/h4&gt;

&lt;p&gt;Unix文件系统提出了链接的概念，若干个文件名可以关联到一个inode节点，inode节点包含一个存储链接数的域。增加一个链接会创建一个目录项并增加inode链接计数。当删除链接时，内核会递减链接计数，为0时删除inode。&lt;/p&gt;

&lt;p&gt;这种类型的链接称为硬链接，只能在一个文件系统中使用，且不能链接到一个目录，避免引起环路。&lt;/p&gt;

&lt;p&gt;另一种链接存在于大多数Unix系操作系统，符号链接：只包含文件名的简单文件，当内核inode转换时遇到符号链接，会将软链接文件内容替换链接名。因为软链接不包含inode，它可以跨文件系统，可以指向任何类型的文件，甚至不存在的文件。但软链接会占用磁盘空间、inode、在路径名到inode转换时引起额外消耗。&lt;/p&gt;

&lt;h4 id=&#34;3-4-设备文件&#34;&gt;3.4 设备文件&lt;/h4&gt;

&lt;p&gt;在Unix系操作系统，设备被当作特殊文件访问。一个设备文件并不占用文件系统空间，只作为设备驱动访问接口。&lt;/p&gt;

&lt;p&gt;有两类特殊文件：字符设备、块设备。主设备号决定类型，次设备号决定哪一个设备。&lt;/p&gt;

&lt;h3 id=&#34;4-vfs&#34;&gt;4 VFS&lt;/h3&gt;

&lt;p&gt;VFS是一个文件系统抽象层，定义了一个文件系统应该实现的操作，对上层屏蔽了底层不同文件系统的实现，一图概之：
&lt;img src=&#34;http://e2fsprogs.sourceforge.net/ext2-vfs.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;5-ext2&#34;&gt;5 EXT2&lt;/h3&gt;

&lt;h4 id=&#34;5-1-起因&#34;&gt;5.1 起因&lt;/h4&gt;

&lt;p&gt;修复EXT文件系统问题，提供一个强大的文件系统，实现unix文件语义并提供高级特性&lt;/p&gt;

&lt;h4 id=&#34;5-2-标准-ext2fs特性&#34;&gt;5.2 标准 ext2fs特性&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;支持标准Unix文件类型：普通文件、目录、设备文件、符号链接&lt;/li&gt;
&lt;li&gt;支持最大4TB文件系统&lt;/li&gt;
&lt;li&gt;长文件名：255字节，可扩展至1012&lt;/li&gt;
&lt;li&gt;为root保留空间以便修复&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;5-3-高级-ext2fs特性&#34;&gt;5.3 高级 ext2fs特性&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;属性继承&lt;/li&gt;
&lt;li&gt;软链接：目标名存储在inode中&lt;/li&gt;
&lt;li&gt;创建文件系统时可选择逻辑块大小&lt;/li&gt;
&lt;li&gt;fsck&lt;/li&gt;
&lt;li&gt;mount options&lt;/li&gt;
&lt;li&gt;Append-only files&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;5-4-物理结构&#34;&gt;5.4 物理结构&lt;/h4&gt;

&lt;p&gt;受BSD文件系统的影响，文件系统由块组构成，但块组并没有与磁盘的物理结构块绑定，因为现代驱动趋势是优化顺序访问和对操作系统隐藏物理结构。&lt;/p&gt;

&lt;p&gt;文件系统物理结构：
&lt;table border=&#34;&#34;&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;Boot&lt;br&gt;Sector&lt;/td&gt;
&lt;td&gt;Block&lt;br&gt;Group 1&lt;/td&gt;
&lt;td&gt;Block&lt;br&gt;Group 2&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;br&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;Block&lt;br&gt;Group N&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;&lt;/p&gt;

&lt;p&gt;每一个块组包含一份冗余的文件系统控制信息（超级块和文件系统描述信息），并包含一部分文件系统（块位图、inode位图、inode表、数据块）：
&lt;table border=&#34;&#34;&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;Super&lt;br&gt;Block&lt;/td&gt;
&lt;td&gt;FS&lt;br&gt;descriptors&lt;/td&gt;
&lt;td&gt;Block&lt;br&gt;Bitmap&lt;/td&gt;
&lt;td&gt;Inode&lt;br&gt;Bitmap&lt;/td&gt;
&lt;td&gt;Inode&lt;br&gt;Table&lt;/td&gt;
&lt;td&gt;Data&lt;br&gt;Blocks&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;&lt;/p&gt;

&lt;p&gt;使用block group提高系统可靠性，冗余超级块信息可以简化文件系统恢复；inode表与数据块一起存储提高寻道时间，提高性能。&lt;/p&gt;

&lt;p&gt;在ext2fs中，目录使用链表管理变长目录项，每一项包含inode号、记录项长度、文件名和文件名长度。通过变长目录项可以实现长文件名同时减少磁盘空间浪费，目录项结构如下：
&lt;table border=&#34;&#34;&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;inode number&lt;/td&gt;&lt;td&gt;entry length&lt;/td&gt;
&lt;td&gt;name length&lt;/td&gt;&lt;td&gt;filename&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;&lt;/p&gt;

&lt;h4 id=&#34;5-5-性能优化&#34;&gt;5.5 性能优化&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;预读：当读取一个块时，内核会请求连续的若干块，当读取下一个块时会从buffer cache读取&lt;/li&gt;
&lt;li&gt;分配优化： 块组的inode、data分配会在当前组中以减少寻道时间&lt;/li&gt;
&lt;li&gt;写时预分配&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;6-ext2fs-library-略&#34;&gt;6 Ext2fs library （略）&lt;/h3&gt;

&lt;p&gt;译自：&lt;a href=&#34;http://e2fsprogs.sourceforge.net/ext2intro.html&#34;&gt;http://e2fsprogs.sourceforge.net/ext2intro.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>认识sk_buff结构体</title>
      <link>http://www.itech.red/2017/04/%E8%AE%A4%E8%AF%86sk_buff%E7%BB%93%E6%9E%84%E4%BD%93/</link>
      <pubDate>Tue, 04 Apr 2017 19:07:48 +0800</pubDate>
      
      <guid>http://www.itech.red/2017/04/%E8%AE%A4%E8%AF%86sk_buff%E7%BB%93%E6%9E%84%E4%BD%93/</guid>
      <description>

&lt;h2 id=&#34;sk-buff结构体&#34;&gt;sk_buff结构体&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;struct sk_buff {
/* These two members must be first. */
struct sk_buff      *next;
struct sk_buff      *prev;
/* sk_buff双向链表 */
ktime_t         tstamp;
/* 报文接收时间戳，是个偏移量 */
struct sock     *sk;
/* 拥有此skb的socket */
struct net_device   *dev;
/* SKB收发的网络设备 */

/*
 * This is the control buffer. It is free to use for every
 * layer. Please put your private variables there. If you
 * want to keep them across layers you have to do a skb_clone()
 * first. This is owned by whoever has the skb queued ATM.
 */
char            cb[48] __aligned(8);
/* 控制缓存区，TCP、buletooth协议都有用到 */

unsigned long       _skb_refdst;
/* 指向dst_entry地址 */
unsigned int        len,
            data_len;
/* len:报文总长度，data_len:数据长度，仅当非线性存储报文时使用 */
__u16           mac_len,
            hdr_len;
/* mac_len:2层头长度，hdr_len:3层头长度*/
union {
    __wsum      csum;
    struct {
        __u16   csum_start;
        __u16   csum_offset;
    };
};
/* 校验和 */
__u32           priority;
/* 报文队列优先级 */
kmemcheck_bitfield_begin(flags1);
/* 位域开始指针 */
__u8            local_df:1,   //分片标志
            cloned:1,         //克隆标志，仅克隆SKB数据共享
            ip_summed:2,      //IP层校验和标志位 CHECKSUM_NONE需软件计算校验和 
            nohdr:1,          //报文头标志位，不需要头时置位
            nfctinfo:3;       //连接追踪，NAT中使用
__u8            pkt_type:3,   //报文类型：多播、广播、单播、非本地
            fclone:2,         //克隆状态
            ipvs_property:1,  //ip virtual server标志，4层负载均衡
            peeked:1,         //是否被查看标志
            nf_trace:1;       //netfilter trace标志
kmemcheck_bitfield_end(flags1);
__be16          protocol;     //协议

void            (*destructor)(struct sk_buff *skb);   //释放skb的回调函数

int         skb_iif;
//接收skb的接口索引
__u32           rxhash;
//源目的IP地址＋port计算hash值，保证SMP下相同flow被同一CPU处理，提高Cache命中率
__be16          vlan_proto;
//vlan协议 802.1q 、802.1ad
__u16           vlan_tci;
//vlan tag:包括低12位id，1位标志位，高3位优先级
__u16           queue_mapping;
//多队列网卡队列映射
kmemcheck_bitfield_begin(flags2);
__u8            pfmemalloc:1;    //标志位，pfmemalloc分配的skb
__u8            ooo_okay:1;      //out of order
__u8            l4_rxhash:1;     //4元组hash
__u8            wifi_acked_valid:1;  
__u8            wifi_acked:1;
__u8            no_fcs:1;        //使网卡SKB最后4字节为帧间序
__u8            head_frag:1;
/* Encapsulation protocol and NIC drivers should use
 * this flag to indicate to each other if the skb contains
 * encapsulated packet or not and maybe use the inner packet
 * headers if needed
 */
__u8            encapsulation:1;   //是否为封装报文，如vxlan
/* 7/9 bit hole (depending on ndisc_nodetype presence) */
kmemcheck_bitfield_end(flags2);
union {
    __u32       mark;            //标记skb，用于netfilter
    __u32       dropcount;          
    __u32       reserved_tailroom;
};

sk_buff_data_t      inner_transport_header;
sk_buff_data_t      inner_network_header;
sk_buff_data_t      inner_mac_header;
sk_buff_data_t      transport_header;
sk_buff_data_t      network_header;
sk_buff_data_t      mac_header;
/* These elements must be at the end, see alloc_skb() for details.  */
sk_buff_data_t      tail;       //指向数据尾  skb_put
sk_buff_data_t      end;        //指向缓存区尾
unsigned char       *head,      //指向缓存区头  
            *data;              //指向数据开头  skb_push skb_pull
unsigned int        truesize;   //总大小 包括skb结构体与block
atomic_t        users;  //引用计数
};
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;skb-shared-info结构体-位于block后-skb-end&#34;&gt;skb_shared_info结构体(位于block后，skb-&amp;gt;end)&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;260 struct skb_shared_info {
261     unsigned char   nr_frags;   //数组frags大小
262     __u8        tx_flags;       //tx标志位
263     unsigned short  gso_size;           
264     /* Warning: this field is not always filled in (UFO)! */
265     unsigned short  gso_segs;
266     unsigned short  gso_type;
267     struct sk_buff  *frag_list;      //分片
268     struct skb_shared_hwtstamps hwtstamps;
269     __be32          ip6_frag_id;
270
271     /*
272      * Warning : all fields before dataref are cleared in __alloc_skb()
273      */
274     atomic_t    dataref;    //引用计数
275
276     /* Intermediate layers must ensure that destructor_arg
277      * remains valid until skb destructor */
278     void *      destructor_arg;
279
280     /* must be last field, see pskb_expand_head() */
281     skb_frag_t  frags[MAX_SKB_FRAGS];
282 };
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://www.itech.red/resume/</link>
      <pubDate>Sat, 01 Apr 2017 18:00:54 +0800</pubDate>
      
      <guid>http://www.itech.red/resume/</guid>
      <description>

&lt;h2 id=&#34;个人信息&#34;&gt;个人信息&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;赵鑫/男/1988&lt;/li&gt;
&lt;li&gt;硕士/电子科技大学计算机与工程学院&lt;/li&gt;
&lt;li&gt;英语能力:CET-6&lt;/li&gt;
&lt;li&gt;联系方式:18458414000&lt;/li&gt;
&lt;li&gt;邮箱:tryit0714@gmail.com&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;工作经历&#34;&gt;工作经历&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;淘宝(中国)软件软件有限公司（2015年7月～至今） 系统工程师&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kernel/OS自动化测试平台 (2016.11~至今)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Kernel/OS自动化测试平台源于我所在的系统软件研发团队，系统软件研发团队负责Linux Kernel研发、维护以及OS相关的安装、YUM、NTP、SYSLOG等基础服务，该平台基于Django、Celery分布式队列实现了模板管理、测试机池管理、基线管理、回归测试、集群测试与单机测试等功能，当代码提交后，gitlab的webhook会发起jenkins完成内核构建调用平台API发起回归测试，当测试完成时对比基线发送测试报告给用户，使内核测试工作更加自动化&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;集团跳板机系统、大盘与高危审计项目 (2016.4-2016.9)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;跳板机系统基于python事件驱动网络框架twisted，实现了基本的SSH服务，并在pam认证中加入了阿里郎验证，集成了ttyrpld内核模块与rdpproxy，记录所有用户操作到日志平台；跳板机大盘与高危审计系统实现了对跳板机实时在线人数、负载的监控、限流与容灾、高危审计等功能；我主要负责ttyrpld内核模块移植与跳板机大盘与高危审计系统的开发，系统已上线运营，为运维及内检提供了一个可靠的跳板机监控与审计平台&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;智能网卡项目 （2015.9-2016.3）&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;智能网卡项目是基础架构事业群的重点项目，该项目主要实现将阿里云内核模块HAVS（高性能阿里虚拟交换机）移植到Cavium 73网卡上，让出更多的CPU资源，提升虚拟比，提升虚拟机带宽，Cavium73网卡为MIPS架构12核，项目中使用1核跑Linux负责与主机的命令通道，其他11核跑HAVS，我在项目中主要负责多vlan支持、vxlan协议的实现、网卡bonding实现、网卡特性支持、性能测试等方面的工作，网卡4核工作时即可达到64Byte包线速14.88Mpps，目前智能网卡已在阿里云机房部署应用&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Linux进程级网络流量监控（2015.7）&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;该项目是阿里云主机监控项目中的一个子问题，阿里云主机监控项目旨在为客户提供一个非侵入性的监控产品，即无需在客户主机安装任何客户端，即可获得主机CPU、内存、IO等基本监控信息；进程级网络流量监控确定了两种方案：一种是基于raw socket嗅探用户态实现，一种是基于netfilter内核模块实现；我主要负责基于netfilter内核模块方案的开发与实现，通过在netfilter两个hook点对进程TCP、UDP报文的统计，将统计信息通过proc文件系统展示，考虑到对用户主机侵入性、安全性等问题线上采用用户态实现&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;其他项目&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;DNS数据分析与一键切换：DNS实时数据分析项目采用metaq＋galaxy实时计算平台对ADNS解析纪录进行实时计算统计主要用于高防清洗与实时请求监控；离线数据分析采用ODPS对阿里公共DNS请求纪录进行计算与统计，用于QPS、热点域名等的统计工作；DNS一键切换利用现有DNS多机房、多安全域切换脚本，为容灾平台提供统一的Restful切换API，使得容灾工作白屏化、简单化&lt;/li&gt;
&lt;li&gt;集团Linux服务器账号系统：该项目旨在建立一个完善的服务器账号生命周期管理系统，采用spring+mybatis+velocity＋Notify等技术与框架，实现了对全集团服务器账号的全面管控，包括账号申请、账号推送、过期提醒、离职注销等各个方面&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;项目经验-2012-7-2015-6-研究生期间&#34;&gt;项目经验(2012.7-2015.6 研究生期间)&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;BANS监控系统(2014.10 - 2015.6)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;该自主神经监控系统负责对云平台，具体包括物理服务器(Debian)、虚拟服务器(Windows)和终端(Ubuntu)进行监控，监控内容包括CPU、内存、硬盘、网络IO、磁盘IO。支持实时显示、历史日志查询、事件告警等功能。虚拟机为神经元，主机为神经中枢，集群作为大脑控制整个云平台的调度，通过监控信息的收集来帮助集群做出决策，监控信息逐层上报，各层具有不同的分析职能。
￼￼￼￼￼￼￼￼￼￼我负责项目架构设计与研发，制定了内部接口规范、内部库的编写以及服务端的开发。开发平台为Linux，利用网络编程、进程通信、线程池等技术实现了神经系统各个部分，不同层级神经组织利用采集信息作出反馈，并将监控信息传给前端做实时数据展示。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;中云操作系统(2012.7 - 2015.7)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;该项目旨在为虚拟桌面云(VDI)提供一个基础设施环境，建立一个统一集中化的管理系统，对平台内的计算资源、存储资源进行管理，有效合理地分配给用户。该平台主要包括服务端、云终端、管控端三个部分。
本人在该项目期间担任主程，利用EPOLL实现异步事件驱动服务端，服务端接受管控端、云终端的命令，实现对资源的分配、管理及使用；利用UNIX域套接字接入KVM命令控制系统完成对虚拟机的冷、热迁移；利用Qt实现了简单浏览器用作云客户端，实现类似windows桌面的功能。&lt;/p&gt;

&lt;h2 id=&#34;技能清单&#34;&gt;技能清单&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;精通Linux平台下C/C++，精通Linux平台服务器端编程&lt;/li&gt;
&lt;li&gt;熟悉Python、Shell脚本语言，熟悉Django框架&lt;/li&gt;
&lt;li&gt;对Linux内核网络协议栈、文件系统、进程管理有一定的研究&lt;/li&gt;
&lt;li&gt;熟悉常用的数据结构、算法和设计模式&lt;/li&gt;
&lt;li&gt;熟悉MySQL数据库&lt;/li&gt;
&lt;li&gt;熟悉Java Web开发,熟悉Spring/Mybatis/Maven常用框架和技术，有开发经验&lt;/li&gt;
&lt;li&gt;熟悉分布式技术，阅读过Redis、Nginx源码&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;致谢&#34;&gt;致谢&lt;/h2&gt;

&lt;p&gt;感谢您花时间阅读我的简历,期待能有机会和您共事。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linux磁盘使用率100%异常排查</title>
      <link>http://www.itech.red/2017/04/linux%E7%A3%81%E7%9B%98%E4%BD%BF%E7%94%A8%E7%8E%87100%E5%BC%82%E5%B8%B8%E6%8E%92%E6%9F%A5/</link>
      <pubDate>Sat, 01 Apr 2017 18:00:54 +0800</pubDate>
      
      <guid>http://www.itech.red/2017/04/linux%E7%A3%81%E7%9B%98%E4%BD%BF%E7%94%A8%E7%8E%87100%E5%BC%82%E5%B8%B8%E6%8E%92%E6%9F%A5/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;缘起&#34;&gt;缘起&lt;/h2&gt;

&lt;p&gt;线上堡垒机系统磁盘根目录/空间使用率100%导致无法添加账号，奇怪的是du查看实际只使用了8G多空间还有40多G，而df查看使用率却是100%，同事找过来让帮忙定位问题。&lt;/p&gt;

&lt;h2 id=&#34;du-h-max-depth-1-x-查看-目录磁盘占用情况-x排除挂载的其他磁盘&#34;&gt;du -h &amp;ndash;max-depth=1 -x 查看/目录磁盘占用情况(-x排除挂载的其他磁盘)&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;http://occ9e8dkg.bkt.clouddn.com/du.png&#34; alt=&#34;du&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;df查看-磁盘使用率在94&#34;&gt;df查看/磁盘使用率在94%&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;http://occ9e8dkg.bkt.clouddn.com/df.png&#34; alt=&#34;df&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;du与df机制对比&#34;&gt;du与df机制对比&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;df读取的是超级块的内容&lt;/li&gt;
&lt;li&gt;du是将所有文件对象大小加起来&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;两种机制的不同会造成两者结果不一致的地方，例如你在命令行下删除了一个文件，而这个文件正在被某个程序打开占用，实际上这个文件依然占用磁盘空间，只有在使用该文件的进程关闭时才真正清楚磁盘空间，这时du显示的数据会比df显示的值小。&lt;/p&gt;

&lt;h2 id=&#34;解决思路一&#34;&gt;解决思路一&lt;/h2&gt;

&lt;p&gt;通过lsof查看是否有已删除的文件仍被进程所使用：&lt;/p&gt;

&lt;p&gt;&lt;code&gt;lsof | grep &#39;(deleted)&#39;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;找到某个大文件对应的进程，关闭该进程可以正确释放磁盘空间
但该方法在本例中并未解决问题！！！&lt;/p&gt;

&lt;h2 id=&#34;解决思路二&#34;&gt;解决思路二&lt;/h2&gt;

&lt;p&gt;在df的图中可以看到，根目录/下还挂载着/home与/home/admin是否有可能是&lt;strong&gt;原根目录下/home目录被挂载的/home覆盖&lt;/strong&gt;，而原/home目录的数据并未清楚，导致磁盘占用&lt;/p&gt;

&lt;p&gt;&lt;code&gt;mount -o bind / /mnt&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;在mnt下果然看到了原来/home目录，该目录下的废弃的用户账户占用磁盘40G空间！ 之前同事增加磁盘做文件系统粗心导致旧的数据未删除，给自己挖了一个坑！&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>